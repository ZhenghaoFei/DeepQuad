diff --git a/.DS_Store b/.DS_Store
index 181a197..8305d60 100644
Binary files a/.DS_Store and b/.DS_Store differ
diff --git a/run_simulator.py b/run_simulator.py
index 99abc8d..cab5b2a 100644
--- a/run_simulator.py
+++ b/run_simulator.py
@@ -45,18 +45,18 @@ def plot_states(states):
     plt.show()
 
 def main():
-    quad  = QuadCopter()
-    time  = 1 # sec
+    quad  = QuadCopter(inverted_pendulum=False)
+    time  = 5 # sec
     steps = int(time/quad.Ts)
-    delta  = [2, 2, 0.9, 0.9]
+    delta  = [2, 2, 2, 2]
 
     print "Simulate %i sec need total %i steps" %(time, steps)
 
     states = np.zeros([steps, quad.stateSpace])
     for i in range(steps):
         state ,_ ,_ = quad.step(delta)
+
         states[i] = state
-    print state
     plot_states(states)
 
 
diff --git a/simulator.py b/simulator.py
index 212f173..4c703aa 100644
--- a/simulator.py
+++ b/simulator.py
@@ -8,29 +8,43 @@ import numpy as np
 from scipy.integrate import odeint
 
 class QuadCopter(object):
-    def __init__(self, Ts=0.01, inverted_pendulum=True):
+    def __init__(self, Ts=0.01, max_time = 10, inverted_pendulum=True):
     # simulator  step time
         self.Ts          = Ts
+        self.max_time = max_time
         self.stateSpace  = 16
         self.actionSpace = 4
-        self.actionLimit  = 2 # maximum rotor speed degree/s TBD
+        self.actionLimit  = 5 # maximum rotor speed degree/s TBD
         self.inverted_pendulum = inverted_pendulum
 
     # physical parameters of airframe
+        # self.gravity = 9.81
+        # self.l       = 0.175  # m, Distance between rotor and center
+        # self.pen_l   = 0.20  # m, the length of stick
+        # self.k1      = 1.0      # propellers constant
+        # self.k2      = 2.0      # propellers constant 
+        # self.mass    = 0.5  # mass
+        # self.Jx      = 2.32e-3
+        # self.Jy      = 2.32e-3
+        # self.Jz      = 4.00e-3
+
         self.gravity = 9.81
-        self.l       = 45.0/1000  # m, Distance between rotor and center
-        self.pen_l   = 45.0/1000  # m, the length of stick
-        self.k1      = 100.0      # propellers constant
-        self.k2      = 100.0      # propellers constant 
-        self.mass    = 28.0/1000  # mass
-        self.Jx      = 16.60e-6
-        self.Jy      = 16.60e-6
-        self.Jz      = 29.26e-6
+        self.l = 0.2; # m, Distance between rotor and center
+        self.pen_l   = 0.20  # m, the length of stick
+        self.k1 = 100; # propellers constant
+        self.k2 = 100; # propellers constant 
+        self.R = 0.04; # m, Center mass radius 
+        self.M = 1 # kg, Body weight
+        self.m = 0.07 #kg, Rotor weight 
+        self.mass = self.M + self.m;
+        self.Jx   = 2*self.M*self.R**2/5 + 2*self.l*self.m;
+        self.Jy   = 2*self.M*self.R**2/5 + 2*self.l*self.m;
+        self.Jz   = 2*self.M*self.R**2/5 + 4*self.l*self.m;
 
     # initial conditions
         self.pn0    = 0.0  # initial North position
         self.pe0    = 0.0  # initial East position
-        self.pd0    = -10.0  # initial Down position (negative altitude)
+        self.pd0    = 0.0  # initial Down position (negative altitude)
         self.u0     = 0.0  # initial velocity along body x-axis
         self.v0     = 0.0  # initial velocity along body y-axis
         self.w0     = 0.0  # initial velocity along body z-axis
@@ -47,6 +61,17 @@ class QuadCopter(object):
         self.pen_vx0   = 0.0 # initial velocity along iv in vehicle frame
         self.pen_vy0   = 0.0 # initial velocity along jv in vehicle frame
 
+    # maximum conditions
+        self.pn_max    =  100  # max North position
+        self.pe_max    =  100  # max East position
+        self.pd_max    =  100  # max Down position (negative altitude)
+        self.u_max     = 10 # max velocity along body x-axis
+        self.v_max     = 10 # max velocity along body y-axis
+        self.w_max     = 10 # max velocity along body z-axis
+        self.p_max     = 10 # max body frame roll rate
+        self.q_max     = 10 # max body frame pitch rate
+        self.r_max     = 10 # max body frame yaw rate
+
     # apply initial conditions
         self.reset()
 
@@ -75,11 +100,11 @@ class QuadCopter(object):
         return self.states
 
     def force(self, x):
-        f = 2.130295e-11*x**2.0 + 1.032633e-6*x + 5.484560e-4
+        f = self.k1 * x
         return f 
 
     def torque(self, x):
-        tau = 0.005964552*self.force(x) + 1.563383e-5
+        tau = self.k2 * x
         return tau
 
     def trunc_error(self,x):
@@ -100,10 +125,11 @@ class QuadCopter(object):
 
         uu = np.asarray([self.trunc_error(Force_x), self.trunc_error(Force_y), self.trunc_error(Force_z),
                          self.trunc_error(Torque_x), self.trunc_error(Torque_y), self.trunc_error(Torque_z)])
-        # print uu
+
         return uu
 
 
+
     def Derivative(self, states, t, delta_f, delta_r, delta_b, delta_l):
     # state variables
         pn     = states[0]    
@@ -213,15 +239,15 @@ class QuadCopter(object):
         terminated = False
         info = 'normal'
 
-        delta   = np.asarray(delta) * 37286.9359183576
+        delta   = np.asarray(delta)*3.1416/180
         delta_f = delta[0]
         delta_r = delta[1]
         delta_b = delta[2]
         delta_l = delta[3]
 
     # integral, ode
-        sol = odeint(self.Derivative, self.states, [self.time, self.time+self.Ts], args=(delta_f,delta_r,delta_b,delta_l), full_output=False, printmessg=False)
-        # sol = self.naive_int(self.Derivative, self.states, self.Ts, [delta_f,delta_r,delta_b,delta_l])
+        # sol = odeint(self.Derivative, self.states, [self.time, self.time+self.Ts], args=(delta_f,delta_r,delta_b,delta_l), full_output=False, printmessg=False)
+        sol = self.naive_int(self.Derivative, self.states, self.Ts, [delta_f,delta_r,delta_b,delta_l])
 
         self.pn     = sol[1,0] 
         self.pe     = sol[1,1] 
@@ -241,11 +267,59 @@ class QuadCopter(object):
         self.pen_vy = sol[1,15]
         self.time   += self.Ts
 
-    # Fail condition check
-        if self.pd0 > 0:
+        # check boundry condition
+        if self.pn > self.pn_max:
+            self.pn = self.pn_max
+        if self.pn < -self.pn_max:
+            self.pn = -self.pn_max
+
+        if self.pe > self.pe_max:
+            self.pe = self.pe_max
+        if self.pe < -self.pe_max:
+            self.pe = -self.pe_max
+
+        if self.pd > self.pd_max:
+            self.pd = self.pd_max
+        if self.pd < -self.pd_max:
+            self.pd = -self.pd_max
+
+        if self.u > self.u_max:
+            self.u = self.u_max
+        if self.u < -self.u_max:
+            self.u = -self.u_max
+
+        if self.v > self.v_max:
+            self.v = self.v_max
+        if self.v < -self.v_max:
+            self.v = -self.v_max
+
+        if self.w > self.w_max:
+            self.w = self.w_max
+        if self.w < -self.w_max:
+            self.w = -self.w_max
+
+        if self.p > self.p_max:
+            self.p = self.p_max
+        if self.p < -self.p_max:
+            self.p = -self.p_max
+
+        if self.q > self.q_max:
+            self.q = self.q_max
+        if self.q < -self.q_max:
+            self.q = -self.q_max
+
+        if self.r > self.r_max:
+            self.r = self.r_max
+        if self.r < -self.r_max:
+            self.r = -self.r_max
+
+        if self.time > self.max_time:
             terminated = True
-            info = 'crash'   
-            print info         
+            info = 'timeout'   
+    # # Fail condition check
+    #     if self.pd > 0:
+    #         terminated = True
+    #         info = 'crash'   
 
         # print 'Time = %f' %self.time
         self.states = np.asarray([self.pn, self.pe, self.pd, self.u, self.v, self.w, self.phi, self.theta, self.psi,
diff --git a/simulator.pyc b/simulator.pyc
index c4e33b1..abe164a 100644
Binary files a/simulator.pyc and b/simulator.pyc differ
diff --git a/train_quadcopter.py b/train_quadcopter.py
index f329d6f..9b69b6f 100644
--- a/train_quadcopter.py
+++ b/train_quadcopter.py
@@ -1,16 +1,14 @@
 # This file is aim to train the quadcopter keet at a constant position
 
 import numpy as np
-import tflearn
 import time
 from simulator import QuadCopter
 from replay_buffer import ReplayBuffer
 from  util import *
+from quad_task import *
 import tensorflow as tf
 import tensorflow.contrib.layers as layers
-import sys, os
-
-
+import tflearn
 
 # ==========================
 #   Training Parameters
@@ -20,17 +18,17 @@ SIM_TIME_STEP = 0.01
 # Max training steps
 MAX_EPISODES = 50000
 # Max episode length
-MAX_EP_TIME = 2 # second
+MAX_EP_TIME = 20 # second
 MAX_EP_STEPS = int(MAX_EP_TIME/SIM_TIME_STEP)
 # Explore decay rate
 EXPLORE_INIT = 1
 EXPLORE_DECAY = 0.99
-EXPLORE_MIN = 0.1
+EXPLORE_MIN = 0.005
 
 # Base learning rate for the Actor network
-ACTOR_LEARNING_RATE = 1e-4
+ACTOR_LEARNING_RATE = 1e-6
 # Base learning rate for the Critic Network
-CRITIC_LEARNING_RATE = 1e-3
+CRITIC_LEARNING_RATE = 1e-5
 # Discount factor 
 GAMMA = 0.99
 # Soft target update param
@@ -42,7 +40,7 @@ TAU = 0.001
 
 # Directory for storing tensorboard summary results
 SUMMARY_DIR = './results/ddpg/'
-SAVE_STEP = 10
+SAVE_STEP = 50
 
 RANDOM_SEED = 1234
 # Size of replay buffer
@@ -95,29 +93,29 @@ class ActorNetwork(object):
         self.num_trainable_vars = len(self.network_params) + len(self.target_network_params)
 
     def create_actor_network(self): 
-        inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim], name='state')
-        net = tf.layers.batch_normalization(inputs)
-        net = layers.fully_connected(inputs, num_outputs=400, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
-        net = tf.layers.batch_normalization(net)
-
-        net = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
-        net = tf.layers.batch_normalization(net)
-        # net = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.th)
-        out_w = tf.Variable(np.random.randn(300, self.a_dim)*3e-3, dtype=tf.float32, name="out_w")
-        out_b = tf.Variable(tf.zeros([self.a_dim]), dtype=tf.float32, name="out_b")
-        out = tf.tanh(tf.matmul(net, out_w) +out_b)
-        scaled_out = tf.multiply(out, self.action_limit)# Scale output to -action_limit to action_limit
-
-        ## tflearn version
-        # inputs = tflearn.input_data(shape=[None, self.s_dim])
-        # net = tflearn.fully_connected(inputs, 400, activation='relu')
-        # net = tflearn.fully_connected(net, 300, activation='relu')
-        # # Final layer weights are init to Uniform[-3e-3, 3e-3]
-        # w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
-        # out = tflearn.fully_connected(
-        #     net, self.a_dim, activation='tanh', weights_init=w_init)
-        # # Scale output to -action_bound to action_bound
-        # scaled_out = tf.multiply(out, self.action_limit)
+        # inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim], name='state')
+        # net = tf.layers.batch_normalization(inputs)
+        # net = layers.fully_connected(inputs, num_outputs=400 ,activation_fn=tf.nn.relu)
+        # net = tf.layers.batch_normalization(net)
+
+        # net = layers.fully_connected(net, num_outputs=300 ,activation_fn=tf.nn.relu)
+        # net = tf.layers.batch_normalization(net)
+        # # net = layers.fully_connected(net, num_outputs=300 ,activation_fn=tf.th)
+        # out_w = tf.Variable(np.random.randn(300, self.a_dim)*3e-3, dtype=tf.float32, name="out_w")
+        # out_b = tf.Variable(tf.zeros([self.a_dim]), dtype=tf.float32, name="out_b")
+        # out = tf.tanh(tf.matmul(net, out_w) + out_b)
+        # scaled_out = tf.multiply(out, self.action_limit)# Scale output to -action_limit to action_limit
+
+        # tflearn version
+        inputs = tflearn.input_data(shape=[None, self.s_dim])
+        net = tflearn.fully_connected(inputs, 400, activation='relu')
+        net = tflearn.fully_connected(net, 300, activation='relu')
+        # Final layer weights are init to Uniform[-3e-3, 3e-3]
+        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
+        out = tflearn.fully_connected(
+            net, self.a_dim, activation='tanh', weights_init=w_init)
+        # Scale output to -action_bound to action_bound
+        scaled_out = tf.multiply(out, self.action_limit)
 
 
         return inputs, out, scaled_out 
@@ -185,41 +183,40 @@ class CriticNetwork(object):
         self.action_grads = tf.gradients(self.out, self.action)
 
     def create_critic_network(self):
-        inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim])
-        action = tf.placeholder(dtype=tf.float32, shape=[None, self.a_dim])
+        # inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim])
+        # action = tf.placeholder(dtype=tf.float32, shape=[None, self.a_dim])
 
-        net = layers.fully_connected(inputs, num_outputs=400, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
-        # Add the action tensor in the 2nd hidden layer
-        # Use two temp layers to get the corresponding weights and biases 
-        t1 = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer(), activation_fn=None)
-        t2 = layers.fully_connected(action, num_outputs=300, weights_initializer=layers.xavier_initializer(), activation_fn=None)
+        # net = layers.fully_connected(inputs, num_outputs=400 ,activation_fn=tf.nn.relu)
+        # # Add the action tensor in the 2nd hidden layer
+        # # Use two temp layers to get the corresponding weights and biases 
+        # t1 = layers.fully_connected(net, num_outputs=300, activation_fn=None)
+        # t2 = layers.fully_connected(action, num_outputs=300, activation_fn=None)
 
-        net = tf.nn.relu(t1 + t2)
-        net = tf.layers.batch_normalization(net)
-        out_w = tf.Variable(np.random.randn(300, 1)*3e-3, dtype=tf.float32)
-        out_b = tf.Variable(tf.zeros([1]), dtype=tf.float32, name="out_b")
+        # net = tf.nn.relu(t1 + t2)
+        # net = tf.layers.batch_normalization(net)
+        # out_w = tf.Variable(np.random.randn(300, 1)*3e-3, dtype=tf.float32)
+        # out_b = tf.Variable(tf.zeros([1]), dtype=tf.float32, name="out_b")
 
-        out = tf.matmul(net, out_w) + out_b
+        # out = tf.matmul(net, out_w) + out_b
 
-        # out = layers.fully_connected(net, num_outputs=1, weights_initializer=layers.xavier_initializer(), activation_fn=None)
 
         # tflearn version
-        # inputs = tflearn.input_data(shape=[None, self.s_dim])
-        # action = tflearn.input_data(shape=[None, self.a_dim])
-        # net = tflearn.fully_connected(inputs, 400, activation='relu')
+        inputs = tflearn.input_data(shape=[None, self.s_dim])
+        action = tflearn.input_data(shape=[None, self.a_dim])
+        net = tflearn.fully_connected(inputs, 400, activation='relu')
 
         # Add the action tensor in the 2nd hidden layer
         # Use two temp layers to get the corresponding weights and biases
-        # t1 = tflearn.fully_connected(net, 300)
-        # t2 = tflearn.fully_connected(action, 300)
+        t1 = tflearn.fully_connected(net, 300)
+        t2 = tflearn.fully_connected(action, 300)
 
-        # net = tflearn.activation(
-        #     tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')
+        net = tflearn.activation(
+            tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')
 
-        # # linear layer connected to 1 output representing Q(s,a)
-        # # Weights are init to Uniform[-3e-3, 3e-3]
-        # w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
-        # out = tflearn.fully_connected(net, 1, weights_init=w_init)
+        # linear layer connected to 1 output representing Q(s,a)
+        # Weights are init to Uniform[-3e-3, 3e-3]
+        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
+        out = tflearn.fully_connected(net, 1, weights_init=w_init)
         return inputs, action, out
 
     def train(self, inputs, action, predicted_q_value):
@@ -255,7 +252,7 @@ class CriticNetwork(object):
 # ===========================
 def build_summaries(): 
     success_rate = tf.Variable(0.)
-    tf.summary.scalar("Success Rate", success_rate)
+    tf.summary.scalar("Reward ", success_rate)
     episode_ave_max_q = tf.Variable(0.)
     tf.summary.scalar("Qmax Value", episode_ave_max_q)
 
@@ -275,35 +272,14 @@ def count_parameters():
         total_parameters += variable_parametes
     print("total_parameters:", total_parameters)
 
-# ===========================
-#   Reward functions
-# ===========================
-def reward_function_hover_decorator(hover_position_set):
-
-    def reward_function_hover(states, terminal, info):
-        # this function is aimed to let the quadcopter hover in a certain position.
-        # e.g
-        # hover_position = np.asarray([0, 0, 0]) # pn = 0, pe = 0, pd = 0
-
-        if terminal:
-            reward = -500
-            print "terminated " , info
-        else:
-            current_position = states[0:3]
-            # reward function = -MSE(current_position, hover_position)
-            reward = -np.mean((current_position - hover_position_set)**2) + 200
-            # print reward
-
-        return reward
 
-    return reward_function_hover
 
 # ===========================
 #   Agent Training
 # ===========================
 
 
-def train(sess, env, actor, critic, reward_fc):
+def train(sess, env, actor, critic, task):
 
     # Set up summary Ops
     summary_ops, summary_vars = build_summaries()
@@ -350,22 +326,29 @@ def train(sess, env, actor, critic, reward_fc):
         states = np.zeros([MAX_EP_STEPS, env.stateSpace])
 
         if i % SAVE_STEP == 0: # save check point every xx episode
-            sess.run(global_step.assign(i))
+            # sess.run(global_step.assign(i))
             save_path = saver.save(sess, SUMMARY_DIR + "model.ckpt" , global_step = i)
             print("Model saved in file: %s" % save_path)
 
         for j in xrange(MAX_EP_STEPS):
 
             # Added exploration noise
-            exp = np.random.rand() * explore * env.actionLimit
+            # exp = np.random.rand(1, 4) * explore * env.actionLimit
+            exp = np.random.rand(1, 4) * explore * env.actionLimit
+
             a = actor.predict(np.reshape(s, (1, 16))) + exp
+            # a = [[2,2,2,2]]
+     
             # a = actor.predict(np.reshape(s, (1, 16))) + (1. / (1. + i))
-
             s2, terminal, info = env.step(a[0])
+            # print 's', s
+            # print 's2', s2
+            # print j
+            # print "action: ", a[0]
+            # print "state: ", s2
             states[j] = s2
 
-            r = reward_fc(s2, terminal, info) # calculate reward basec on s2
-
+            r = task.reward(s2, terminal, info) # calculate reward basec on s2
             replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r, \
                 terminal, np.reshape(s2, (actor.s_dim,)))
 
@@ -399,40 +382,40 @@ def train(sess, env, actor, critic, reward_fc):
                 actor.update_target_network()
                 critic.update_target_network()
 
-            s = s2
             ep_reward += r
 
-            if terminal or j == MAX_EP_STEPS-1 or r < -10000:
+            if terminal :
                 # if i > 30:
                 #     plot_states(states)
 
                 print s[0:3]
                 time_gap = time.time() - tic
 
-                # summary_str = sess.run(summary_ops, feed_dict={
-                #     summary_vars[0]: ep_reward,
-                #     summary_vars[1]: ep_ave_max_q / float(j)
-                # })
+                summary_str = sess.run(summary_ops, feed_dict={
+                    summary_vars[0]: (ep_reward/(j+1)),
+                    summary_vars[1]: (ep_ave_max_q / float(j+1)),
+                })
 
-                # writer.add_summary(summary_str, i)
-                # writer.flush()
+                writer.add_summary(summary_str, i)
+                writer.flush()
                 # if ep_reward < last_epreward and last_epreward != 0:
                 #     actor.learning_rate /= 10
                 #     critic.learning_rate /= 10
                 #     print "lr decay to ", actor.learning_rate
                 last_epreward = ep_reward
-                print '| Reward: %.2f' % int(ep_reward/(j+1)), " | Episode", i, \
+                print '| Reward: %.2f' % (ep_reward/(j+1)), " | Episode", i, \
                         '| Qmax: %.4f' % (ep_ave_max_q / float(j+1)), ' | Time: %.2f' %(time_gap)
                 tic = time.time()
 
                 break
+            s = np.copy(s2)
 
 
 def main(_):
         
         np.random.seed(RANDOM_SEED)
         tf.set_random_seed(RANDOM_SEED)
-        env  = QuadCopter(SIM_TIME_STEP, inverted_pendulum=False)
+        env  = QuadCopter(SIM_TIME_STEP, max_time = 2, inverted_pendulum=False)
 
         state_dim = env.stateSpace
         action_dim = env.actionSpace
@@ -445,8 +428,8 @@ def main(_):
         print('max time: ', MAX_EP_TIME)
         print('max step: ',MAX_EP_STEPS)        
 
-        hover_position = np.asarray([0, 0, -50])
-        reward_fc = reward_function_hover_decorator(hover_position)
+        hover_position = np.asarray([0, 0, -10])
+        task = hover(hover_position)
         config = tf.ConfigProto()
         config.gpu_options.allow_growth = True
 
@@ -457,7 +440,7 @@ def main(_):
             critic = CriticNetwork(sess, state_dim, action_dim, \
                 CRITIC_LEARNING_RATE, TAU, actor.get_num_trainable_vars())
 
-            train(sess, env, actor, critic, reward_fc)
+            train(sess, env, actor, critic, task)
 
 if __name__ == '__main__':
     tf.app.run()
diff --git a/train_quadcopter_horizontalmove.py b/train_quadcopter_horizontalmove.py
deleted file mode 100644
index f37f481..0000000
--- a/train_quadcopter_horizontalmove.py
+++ /dev/null
@@ -1,464 +0,0 @@
-# This file is aim to train the quadcopter keet at a constant position
-
-import numpy as np
-import tflearn
-import time
-from simulator import QuadCopter
-from replay_buffer import ReplayBuffer
-from  util import *
-import tensorflow as tf
-import tensorflow.contrib.layers as layers
-import sys, os
-
-
-
-# ==========================
-#   Training Parameters
-# =========================
-# Simulation step
-SIM_TIME_STEP = 0.01
-# Max training steps
-MAX_EPISODES = 50000
-# Max episode length
-MAX_EP_TIME = 3 # second
-MAX_EP_STEPS = int(MAX_EP_TIME/SIM_TIME_STEP)
-# Explore decay rate
-EXPLORE_INIT = 1
-EXPLORE_DECAY = 0.99
-EXPLORE_MIN = 0.1
-
-# Base learning rate for the Actor network
-ACTOR_LEARNING_RATE = 1e-4
-# Base learning rate for the Critic Network
-CRITIC_LEARNING_RATE = 1e-3
-# Discount factor 
-GAMMA = 0.99
-# Soft target update param
-TAU = 0.001
-
-# ===========================
-#   Utility Parameters
-# ===========================
-
-# Directory for storing tensorboard summary results
-SUMMARY_DIR = './results/horizontal/'
-SAVE_STEP = 10
-
-RANDOM_SEED = 1234
-# Size of replay buffer
-BUFFER_SIZE = 1e6
-MINIBATCH_SIZE = 64
-
-# ===========================
-#   Actor and Critic DNNs
-# ===========================
-class ActorNetwork(object):
-    """ 
-    Input to the network is the state, output is the action
-    under a deterministic policy.
-
-    """
-    def __init__(self, sess, state_dim, action_dim, action_limit, learning_rate, tau):
-        self.sess = sess
-        self.s_dim = state_dim
-        self.a_dim = action_dim
-        self.action_limit = action_limit
-        self.learning_rate = learning_rate
-        self.tau = tau
-
-        # Actor Network
-        self.inputs, self.out, self.scaled_out = self.create_actor_network()
-
-        self.network_params = tf.trainable_variables()
-
-        # Target Network
-        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()
-        
-        self.target_network_params = tf.trainable_variables()[len(self.network_params):]
-
-        # Op for periodically updating target network with online network weights
-        self.update_target_network_params = \
-            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) + \
-                tf.multiply(self.target_network_params[i], 1. - self.tau))
-                for i in range(len(self.target_network_params))]
-
-        # This gradient will be provided by the critic network
-        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])
-        
-        # Combine the gradients here 
-        self.actor_gradients = tf.gradients(self.scaled_out, self.network_params, -self.action_gradient)
-
-        # Optimization Op
-        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\
-            apply_gradients(zip(self.actor_gradients, self.network_params))
-
-        self.num_trainable_vars = len(self.network_params) + len(self.target_network_params)
-
-    def create_actor_network(self): 
-        inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim], name='state')
-        net = tf.layers.batch_normalization(inputs)
-        net = layers.fully_connected(inputs, num_outputs=400, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
-        net = tf.layers.batch_normalization(net)
-
-        net = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
-        net = tf.layers.batch_normalization(net)
-        # net = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.th)
-        out_w = tf.Variable(np.random.randn(300, self.a_dim)*3e-3, dtype=tf.float32, name="out_w")
-        out_b = tf.Variable(tf.zeros([self.a_dim]), dtype=tf.float32, name="out_b")
-        out = tf.tanh(tf.matmul(net, out_w) +out_b)
-        scaled_out = tf.multiply(out, self.action_limit)# Scale output to -action_limit to action_limit
-
-        ## tflearn version
-        # inputs = tflearn.input_data(shape=[None, self.s_dim])
-        # net = tflearn.fully_connected(inputs, 400, activation='relu')
-        # net = tflearn.fully_connected(net, 300, activation='relu')
-        # # Final layer weights are init to Uniform[-3e-3, 3e-3]
-        # w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
-        # out = tflearn.fully_connected(
-        #     net, self.a_dim, activation='tanh', weights_init=w_init)
-        # # Scale output to -action_bound to action_bound
-        # scaled_out = tf.multiply(out, self.action_limit)
-
-
-        return inputs, out, scaled_out 
-
-    def train(self, inputs, a_gradient):
-        self.sess.run(self.optimize, feed_dict={
-            self.inputs: inputs,
-            self.action_gradient: a_gradient
-        })
-
-    def predict(self, inputs):
-        return self.sess.run(self.scaled_out, feed_dict={
-            self.inputs: inputs
-        })
-
-    def predict_target(self, inputs):
-        return self.sess.run(self.target_scaled_out, feed_dict={
-            self.target_inputs: inputs
-        })
-
-    def update_target_network(self):
-        self.sess.run(self.update_target_network_params)
-
-    def get_num_trainable_vars(self):
-        return self.num_trainable_vars
-
-class CriticNetwork(object):
-    """ 
-    Input to the network is the state and action, output is Q(s,a).
-    The action must be obtained from the output of the Actor network.
-
-    """
-    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, num_actor_vars):
-        self.sess = sess
-        self.s_dim = state_dim
-        self.a_dim = action_dim
-        self.learning_rate = learning_rate
-        self.tau = tau
-
-        # Create the critic network
-        self.inputs, self.action, self.out = self.create_critic_network()
-
-        self.network_params = tf.trainable_variables()[num_actor_vars:]
-
-        # Target Network
-        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()
-        
-        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]
-
-        # Op for periodically updating target network with online network weights with regularization
-        self.update_target_network_params = \
-            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) + tf.multiply(self.target_network_params[i], 1. - self.tau))
-                for i in range(len(self.target_network_params))]
-    
-        # Network target (y_i)
-        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])
-
-        # Define loss and optimization Op
-        self.loss = tf.losses.mean_squared_error(self.predicted_q_value, self.out)
-        # self.loss = tflearn.mean_square(self.predicted_q_value, self.out)
-
-        self.optimize = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)
-
-        # Get the gradient of the net w.r.t. the action
-        self.action_grads = tf.gradients(self.out, self.action)
-
-    def create_critic_network(self):
-        inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim])
-        action = tf.placeholder(dtype=tf.float32, shape=[None, self.a_dim])
-
-        net = layers.fully_connected(inputs, num_outputs=400, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
-        # Add the action tensor in the 2nd hidden layer
-        # Use two temp layers to get the corresponding weights and biases 
-        t1 = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer(), activation_fn=None)
-        t2 = layers.fully_connected(action, num_outputs=300, weights_initializer=layers.xavier_initializer(), activation_fn=None)
-
-        net = tf.nn.relu(t1 + t2)
-        net = tf.layers.batch_normalization(net)
-        out_w = tf.Variable(np.random.randn(300, 1)*3e-3, dtype=tf.float32)
-        out_b = tf.Variable(tf.zeros([1]), dtype=tf.float32, name="out_b")
-
-        net = tf.matmul(net, out_w) + out_b
-        net = tf.layers.batch_normalization(net)
-
-        out = layers.fully_connected(net, num_outputs=1, weights_initializer=layers.xavier_initializer(), activation_fn=None)
-
-        # tflearn version
-        # inputs = tflearn.input_data(shape=[None, self.s_dim])
-        # action = tflearn.input_data(shape=[None, self.a_dim])
-        # net = tflearn.fully_connected(inputs, 400, activation='relu')
-
-        # Add the action tensor in the 2nd hidden layer
-        # Use two temp layers to get the corresponding weights and biases
-        # t1 = tflearn.fully_connected(net, 300)
-        # t2 = tflearn.fully_connected(action, 300)
-
-        # net = tflearn.activation(
-        #     tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')
-
-        # # linear layer connected to 1 output representing Q(s,a)
-        # # Weights are init to Uniform[-3e-3, 3e-3]
-        # w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
-        # out = tflearn.fully_connected(net, 1, weights_init=w_init)
-        return inputs, action, out
-
-    def train(self, inputs, action, predicted_q_value):
-        return self.sess.run([self.out, self.optimize], feed_dict={
-            self.inputs: inputs,
-            self.action: action,
-            self.predicted_q_value: predicted_q_value
-        })
-
-    def predict(self, inputs, action):
-        return self.sess.run(self.out, feed_dict={
-            self.inputs: inputs,
-            self.action: action
-        })
-
-    def predict_target(self, inputs, action):
-        return self.sess.run(self.target_out, feed_dict={
-            self.target_inputs: inputs,
-            self.target_action: action
-        })
-
-    def action_gradients(self, inputs, actions): 
-        return self.sess.run(self.action_grads, feed_dict={
-            self.inputs: inputs,
-            self.action: actions
-        })
-
-    def update_target_network(self):
-        self.sess.run(self.update_target_network_params)
-
-# ===========================
-#   Tensorflow Summary Ops
-# ===========================
-def build_summaries(): 
-    success_rate = tf.Variable(0.)
-    tf.summary.scalar("Success Rate", success_rate)
-    episode_ave_max_q = tf.Variable(0.)
-    tf.summary.scalar("Qmax Value", episode_ave_max_q)
-
-    summary_vars = [success_rate, episode_ave_max_q]
-    summary_ops = tf.summary.merge_all()
-
-    return summary_ops, summary_vars
-
-def count_parameters():
-    total_parameters = 0
-    for variable in tf.trainable_variables():
-        # shape is an array of tf.Dimension
-        shape = variable.get_shape()
-        variable_parametes = 1
-        for dim in shape:
-            variable_parametes *= dim.value
-        total_parameters += variable_parametes
-    print("total_parameters:", total_parameters)
-
-# ===========================
-#   Reward functions
-# ===========================
-def reward_function_hover_decorator(hover_position_set):
-
-    def reward_function_hover(states, terminal, info):
-        # this function is aimed to let the quadcopter hover in a certain position.
-        # e.g
-        # hover_position = np.asarray([0, 0, 0]) # pn = 0, pe = 0, pd = 0
-
-        if terminal:
-            reward = -500
-            print "terminated " , info
-        else:
-            current_position = states[0:3]
-            # reward function = -MSE(current_position, hover_position)
-            reward = -np.mean((current_position - hover_position_set)**2) + 200
-            # print reward
-
-        return reward
-
-    return reward_function_hover
-
-# ===========================
-#   Agent Training
-# ===========================
-
-
-def train(sess, env, actor, critic, reward_fc):
-
-    # Set up summary Ops
-    summary_ops, summary_vars = build_summaries()
-    global_step = tf.Variable(0, dtype=tf.int32)
-
-    sess.run(tf.global_variables_initializer())
-    writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)
-
-    # load model if have
-    saver = tf.train.Saver()
-    checkpoint = tf.train.get_checkpoint_state(SUMMARY_DIR)
-    
-    if checkpoint and checkpoint.model_checkpoint_path:
-        saver.restore(sess, checkpoint.model_checkpoint_path)
-        print ("Successfully loaded:", checkpoint.model_checkpoint_path)
-        print("global step: ", global_step.eval())
-
-    else:
-        print ("Could not find old network weights")
-
-    # Initialize target network weights
-    actor.update_target_network()
-    critic.update_target_network()
-    count_parameters()
-
-    # Initialize replay memory
-    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)
-    tic = time.time()
-    last_epreward = 0 
-    i = global_step.eval()
-
-    while True:
-        i += 1
-        if i > MAX_EPISODES:
-            break
-        print ("Iteration: ", i)
-        explore = EXPLORE_INIT*EXPLORE_DECAY**i
-        explore = max(EXPLORE_MIN, explore)
-        print ("explore: ", explore)
-        s = env.reset()
-
-        ep_reward = 0
-        ep_ave_max_q = 0
-        states = np.zeros([MAX_EP_STEPS, env.stateSpace])
-
-        if i % SAVE_STEP == 0: # save check point every xx episode
-            sess.run(global_step.assign(i))
-            save_path = saver.save(sess, SUMMARY_DIR + "model.ckpt" , global_step = i)
-            print("Model saved in file: %s" % save_path)
-
-        for j in xrange(MAX_EP_STEPS):
-
-            # Added exploration noise
-            exp = np.random.rand() * explore * env.actionLimit
-            a = actor.predict(np.reshape(s, (1, 16))) + exp
-            # a = actor.predict(np.reshape(s, (1, 16))) + (1. / (1. + i))
-
-            s2, terminal, info = env.step(a[0])
-            states[j] = s2
-
-            r = reward_fc(s2, terminal, info) # calculate reward basec on s2
-
-            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r, \
-                terminal, np.reshape(s2, (actor.s_dim,)))
-
-            # Keep adding experience to the memory until
-            # there are at least minibatch size samples
-            if replay_buffer.size() > MINIBATCH_SIZE:     
-                s_batch, a_batch, r_batch, t_batch, s2_batch = \
-                    replay_buffer.sample_batch(MINIBATCH_SIZE)
-
-                # Calculate targets
-                target_q = critic.predict_target(s2_batch, actor.predict_target(s2_batch))
-
-                y_i = []
-                for k in xrange(MINIBATCH_SIZE):
-                    if t_batch[k]:
-                        y_i.append(r_batch[k])
-                    else:
-                        y_i.append(r_batch[k] + GAMMA * target_q[k])
-
-                # Update the critic given the targets
-                predicted_q_value, _ = critic.train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)))
-            
-                ep_ave_max_q += np.amax(predicted_q_value)
-
-                # Update the actor policy using the sampled gradient
-                a_outs = actor.predict(s_batch)                
-                grads = critic.action_gradients(s_batch, a_outs)
-                actor.train(s_batch, grads[0])
-
-                # Update target networks
-                actor.update_target_network()
-                critic.update_target_network()
-
-            s = s2
-            ep_reward += r
-
-            if terminal or j == MAX_EP_STEPS-1 or r < -10000:
-                # if i > 30:
-                #     plot_states(states)
-
-                print s[0:3]
-                time_gap = time.time() - tic
-
-                # summary_str = sess.run(summary_ops, feed_dict={
-                #     summary_vars[0]: ep_reward,
-                #     summary_vars[1]: ep_ave_max_q / float(j)
-                # })
-
-                # writer.add_summary(summary_str, i)
-                # writer.flush()
-                # if ep_reward < last_epreward and last_epreward != 0:
-                #     actor.learning_rate /= 10
-                #     critic.learning_rate /= 10
-                #     print "lr decay to ", actor.learning_rate
-                last_epreward = ep_reward
-                print '| Reward: %.2f' % int(ep_reward/(j+1)), " | Episode", i, \
-                        '| Qmax: %.4f' % (ep_ave_max_q / float(j+1)), ' | Time: %.2f' %(time_gap)
-                tic = time.time()
-
-                break
-
-
-def main(_):
-        
-        np.random.seed(RANDOM_SEED)
-        tf.set_random_seed(RANDOM_SEED)
-        env  = QuadCopter(SIM_TIME_STEP, inverted_pendulum=False)
-
-        state_dim = env.stateSpace
-        action_dim = env.actionSpace
-        action_limit = env.actionLimit
-
-        print("Quadcopter created")
-        print('state_dim: ', state_dim)
-        print('action_dim: ', action_dim)
-        print('action_limit: ',action_limit)
-        print('max time: ', MAX_EP_TIME)
-        print('max step: ',MAX_EP_STEPS)        
-
-        hover_position = np.asarray([10, 10, 0])
-        reward_fc = reward_function_hover_decorator(hover_position)
-        config = tf.ConfigProto()
-        config.gpu_options.allow_growth = True
-
-        with tf.Session(config=config) as sess:
-            actor = ActorNetwork(sess, state_dim, action_dim, action_limit, \
-                ACTOR_LEARNING_RATE, TAU)
-
-            critic = CriticNetwork(sess, state_dim, action_dim, \
-                CRITIC_LEARNING_RATE, TAU, actor.get_num_trainable_vars())
-
-            train(sess, env, actor, critic, reward_fc)
-
-if __name__ == '__main__':
-    tf.app.run()
