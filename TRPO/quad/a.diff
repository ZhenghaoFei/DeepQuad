diff --git a/.DS_Store b/.DS_Store
index 204747c..d579998 100644
Binary files a/.DS_Store and b/.DS_Store differ
diff --git a/DQN/simulator_dis.py b/DQN/simulator_dis.py
index 161f0d1..72b9c5d 100644
--- a/DQN/simulator_dis.py
+++ b/DQN/simulator_dis.py
@@ -71,7 +71,7 @@ class QuadCopter(object):
         self.p_max     = 10 # max body frame roll rate
         self.q_max     = 10 # max body frame pitch rate
         self.r_max     = 10 # max body frame yaw rate
-        self.uu = np.zeros(6, dtype=np.float32)
+        
 
     # apply initial conditions
         self.reset()
@@ -95,6 +95,7 @@ class QuadCopter(object):
         self.pen_vx = self.pen_vx0
         self.pen_vy = self.pen_vy0
         self.time   = 0.0 # simulation time
+        self.uu = np.zeros(6, dtype=np.float32)
 
         self.states = np.asarray([self.pn, self.pe, self.pd, self.u, self.v, self.w, self.phi, self.theta, self.psi,
                                   self.p,  self.q,  self.r,  self.pen_x,  self.pen_y,  self.pen_vx,  self.pen_vy])
@@ -130,18 +131,20 @@ class QuadCopter(object):
         return uu
 
     def action_trans(self, a):
-        delta = 0.2
+        delta = 0.5
         if a == 0:
             self.uu[0] += delta;
         if a == 1:
             self.uu[0] -= delta;
+
         if a == 2:
             self.uu[1] += delta;
         if a == 3:
             self.uu[1] -= delta; 
-        if a == 2:
+
+        if a == 4:
             self.uu[2] += delta;
-        if a == 3:
+        if a == 5:
             self.uu[2] -= delta;  
 
         return self.uu
@@ -172,7 +175,9 @@ class QuadCopter(object):
         taup  = uu[3] #tau phi
         taut  = uu[4] #tau theta
         taus  = uu[5] #tau psi
-    
+        
+        # print fx, fy, fz
+
         sp = np.sin(phi)
         cp = np.cos(phi)
         st = np.sin(theta)
@@ -248,6 +253,7 @@ class QuadCopter(object):
         return states_dot
 
     def naive_int(self, derivative_func, states, Ts, uu):
+
         states_dot = derivative_func(states, Ts, uu)
         states += states_dot*Ts
         sol =  np.vstack((states_dot, states))
@@ -259,8 +265,7 @@ class QuadCopter(object):
         self.info = 'normal'
 
         # procecss action to uu
-        action_trans(action)
-        
+        self.action_trans(action)
         # delta   = np.asarray(delta)*3.1416/180
         # delta_f = delta[0]
         # delta_r = delta[1]
diff --git a/DQN/train_DQN_plain.py b/DQN/train_DQN_plain.py
deleted file mode 100644
index 65d8a8e..0000000
--- a/DQN/train_DQN_plain.py
+++ /dev/null
@@ -1,320 +0,0 @@
-# This version works on 16*16
-
-import tensorflow as tf
-
-import numpy as np
-import tflearn
-# import matplotlib.pyplot as plt
-import time
-
-from replay_buffer import ReplayBuffer
-from simulator_gymstyle_old import *
-
-# ==========================
-#   Training Parameters
-# ==========================
-
-# Max episode length    
-MAX_EP_STEPS = 100
-
-# Base learning rate for the Qnet Network
-Q_LEARNING_RATE = 1e-4
-# Discount factor 
-GAMMA = 0.9
-
-# Soft target update param
-TAU = 0.001
-TARGET_UPDATE_STEP = 100
-
-MINIBATCH_SIZE = 1024
-SAVE_STEP = 10000
-EPS_MIN = 0.05
-EPS_DECAY_RATE = 0.99999
-# ===========================
-#   Utility Parameters
-# ===========================
-# map size
-MAP_SIZE  = 8
-PROBABILITY = 0.1
-# Directory for storing tensorboard summary results
-SUMMARY_DIR = './results_dqn_plain/'
-RANDOM_SEED = 1234
-# Size of replay buffer
-BUFFER_SIZE = 1000000
-EVAL_EPISODES = 100
-
-
-# ===========================
-#   Q DNN
-# ===========================
-class QNetwork(object):
-    """ 
-    Input to the network is the state and action, output is Q(s,a).
-    The action must be obtained from the output of the Actor network.
-
-    """
-    def __init__(self, sess, state_dim, action_dim, learning_rate, tau):
-        self.sess = sess
-        self.s_dim = state_dim
-        self.a_dim = action_dim
-        self.learning_rate = learning_rate
-        self.tau = tau
-
-        # Create the Qnet network
-        self.inputs, self.out = self.create_Q_network()
-
-        self.network_params = tf.trainable_variables()
-
-        # Target Network
-        self.target_inputs, self.target_out = self.create_Q_network()
-        
-        self.target_network_params = tf.trainable_variables()[(len(self.network_params)):]
-
-
-        # Op for periodically updating target network with online network weights with regularization
-        self.update_target_network_params = \
-            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) + tf.multiply(self.target_network_params[i], 1. -self.tau))
-                for i in range(len(self.target_network_params))]
-    
-        # Network target (y_i)
-        self.observed_q_value = tf.placeholder(tf.float32, [None])
-        self.action_taken = tf.placeholder(tf.float32, [None, self.a_dim])
-        self.predicted_q_value = tf.reduce_sum(tf.multiply(self.out, self.action_taken), reduction_indices = 1) 
-
-        # Define loss and optimization Op
-        self.Qnet_global_step = tf.Variable(0, name='Qnet_global_step', trainable=False)
-
-        self.loss = tf.reduce_mean(tf.square(self.predicted_q_value - self.observed_q_value))
-        self.optimize = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.Qnet_global_step)
-
-
-    def create_Q_network(self):
-        inputs = tflearn.input_data(shape=self.s_dim)
-
-        net = tflearn.conv_2d(inputs, 8, 1, activation='relu', name='conv1')
-        net = tflearn.conv_2d(net, 16, 3, activation='relu', name='conv2')
-        # net = tflearn.layers.conv.max_pool_2d (net, 2, strides=None, padding='same', name='MaxPool2D1')
-
-        net = tflearn.conv_2d(net, 8, 3, activation='relu', name='conv3')
-        # net = tflearn.conv_2d(inputs, 8, 3, activation='relu', name='conv3')
-
-        # net = tflearn.conv_2d(net, 16, 3, activation='relu', name='conv2')
-        net = tflearn.fully_connected(net, 64, activation='relu')
-        # net = tflearn.layers.normalization.batch_normalization(net)
-        net = tflearn.fully_connected(net, 32, activation='relu')
-
-        net = tflearn.fully_connected(net, 16, activation='relu')
-        # linear layer connected to 1 output representing Q(s,a) 
-        # Weights are init to Uniform[-3e-3, 3e-3]
-        # w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
-        out = tflearn.fully_connected(net, self.a_dim, activation='tanh')
-        return inputs, out
-
-    def train(self, inputs, action, observed_q_value):
-
-        return self.sess.run([self.out, self.optimize], feed_dict={
-            self.inputs: inputs,
-            self.action_taken: action,
-            self.observed_q_value: observed_q_value
-        })
-
-    def predict(self, inputs):
-        return self.sess.run(self.out, feed_dict={
-            self.inputs: inputs,
-        })
-
-    def predict_target(self, inputs):
-        return self.sess.run(self.target_out, feed_dict={
-            self.target_inputs: inputs,
-        })
-
-    def update_target_network(self):
-        self.sess.run(self.update_target_network_params)
-
-# ===========================
-#   Tensorflow Summary Ops
-# ===========================
-def build_summaries(): 
-    success_rate = tf.Variable(0.)
-    tf.summary.scalar('Success Rate', success_rate)
-    episode_ave_max_q = tf.Variable(0.)
-    tf.summary.scalar('Qmax Value', episode_ave_max_q)
-
-    summary_vars = [success_rate, episode_ave_max_q]
-    summary_ops = tf.summary.merge_all()
-
-    return summary_ops, summary_vars
-
-# ===========================
-#   Agent Training
-# ===========================
-def train(sess, env, Qnet, global_step):
-
-    # Set up summary Ops
-    summary_ops, summary_vars = build_summaries()
-
-    sess.run(tf.global_variables_initializer())
-
-    # load model if have
-    saver = tf.train.Saver()
-    checkpoint = tf.train.get_checkpoint_state(SUMMARY_DIR)
-    
-    if checkpoint and checkpoint.model_checkpoint_path:
-        saver.restore(sess, checkpoint.model_checkpoint_path)
-        print ("Successfully loaded:", checkpoint.model_checkpoint_path)
-        print("global step: ", global_step.eval())
-
-    else:
-        print ("Could not find old network weights")
-
-    writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)
-
-    # Initialize target network weights
-    Qnet.update_target_network()
-
-    # Initialize replay memory
-    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)
-
-    i = global_step.eval()
-
-
-    eval_acc_reward = 0
-    tic = time.time()
-    eps = 1
-    while True:
-        i += 1
-        eps = EPS_DECAY_RATE**i
-        eps = max(eps, EPS_MIN)
-        s = env.reset()
-        # plt.imshow(s, interpolation='none')
-        # plt.show()
-        # s = prepro(s)
-        ep_ave_max_q = 0
-
-        if i % SAVE_STEP == 0 : # save check point every 1000 episode
-            sess.run(global_step.assign(i))
-            save_path = saver.save(sess, SUMMARY_DIR + "model.ckpt" , global_step = global_step)
-            print("Model saved in file: %s" % save_path)
-            print("Successfully saved global step: ", global_step.eval())
-
-
-        for j in xrange(MAX_EP_STEPS):
-            predicted_q_value = Qnet.predict(np.reshape(s, np.hstack((1, Qnet.s_dim))))
-            predicted_q_value = predicted_q_value[0]
-
-            np.random.seed()
-
-            action = np.argmax(predicted_q_value)
-            if np.random.rand() < eps:
-                action = np.random.randint(4)
-                # print('eps')
-            # print'actionprob:', action_prob
-
-            # print(action)
-            # print(a)
-
-            s2, r, terminal, info = env.step(action)
-            # print r, info
-            # plt.imshow(s2, interpolation='none')
-            # plt.show()
-
-            # s2 = prepro(s2)
-
-            # print(np.reshape(s, (actor.s_dim,)).shape)
-            action_vector = action_ecoder(action, Qnet.a_dim)
-            replay_buffer.add(np.reshape(s, (Qnet.s_dim)), np.reshape(action_vector, (Qnet.a_dim)), r, \
-                terminal, np.reshape(s2, (Qnet.s_dim)))
-
-            s = s2
-            eval_acc_reward += r
-
-            if terminal:
-                # Keep adding experience to the memory until
-                # there are at least minibatch size samples
-                if replay_buffer.size() > MINIBATCH_SIZE:     
-                    s_batch, a_batch, r_batch, t_batch, s2_batch = \
-                        replay_buffer.sample_batch(MINIBATCH_SIZE)
-
-                    # Calculate targets
-                    target_q = Qnet.predict_target(s2_batch)
-                    y_i = []
-                    for k in xrange(MINIBATCH_SIZE):
-                        if t_batch[k]:
-                            y_i.append(r_batch[k])
-                        else:
-                            y_i.append(r_batch[k] + GAMMA * np.max(target_q[k]))
-
-                    # # Update the Qnet given the target
-                    predicted_q_value, _ = Qnet.train(s_batch, a_batch, y_i)
-                
-                    ep_ave_max_q += np.amax(predicted_q_value)
-
-                    # Update the actor policy using the sampled gradient
-
-                # Update target networks every 1000 iter
-                # if i%TARGET_UPDATE_STEP == 0:
-                    Qnet.update_target_network()
-
-                if i%EVAL_EPISODES == 0:
-                    # summary
-                    time_gap = time.time() - tic
-                    summary_str = sess.run(summary_ops, feed_dict={
-                        summary_vars[0]: (eval_acc_reward+EVAL_EPISODES)/2,
-                        summary_vars[1]: ep_ave_max_q / float(j+1),
-                    })
-                    writer.add_summary(summary_str, i)
-                    writer.flush()
-
-                    print ('| Success: %i %%' % ((eval_acc_reward+EVAL_EPISODES)/2), "| Episode", i, \
-                        '| Qmax: %.4f' % (ep_ave_max_q / float(j+1)), ' | Time: %.2f' %(time_gap), ' | Eps: %.2f' %(eps))
-                    tic = time.time()
-
-                    # print(' 100 round reward: ', eval_acc_reward)
-                    eval_acc_reward = 0
-
-                break
-
-
-def prepro(state):
-    """ prepro state to 3D tensor   """
-    # print('before: ', state.shape)
-    state = state.reshape(state.shape[0], state.shape[1], 1)
-    # print('after: ', state.shape)
-    # plt.imshow(state, interpolation='none')
-    # plt.show()
-    # state = state.astype(np.float).ravel()
-    return state
-
-def action_ecoder(action, action_dim):
-    action_vector = np.zeros(action_dim, dtype=np.float32)
-    action_vector[action] = 1
-    return action_vector
-
-
-def main(_):
-    config = tf.ConfigProto()
-    config.gpu_options.allow_growth = True
-
-    with tf.Session(config=config) as sess:
- 
-        global_step = tf.Variable(0, name='global_step', trainable=False)
-
-        env = sim_env(MAP_SIZE, PROBABILITY) # creat 
-        # np.random.seed(RANDOM_SEED)
-        # tf.set_random_seed(RANDOM_SEED)
-
-        # state_dim = np.prod(env.observation_space.shape)
-        state_dim = [env.state_dim[0], env.state_dim[1], 1]
-        print('state_dim:',state_dim)
-        action_dim = env.action_dim
-        print('action_dim:',action_dim)
-
-
-        Qnet = QNetwork(sess, state_dim, action_dim, \
-            Q_LEARNING_RATE, TAU)
-
-
-        train(sess, env, Qnet, global_step)
-
-if __name__ == '__main__':
-    tf.app.run()
diff --git a/DeepRL-Agents b/DeepRL-Agents
--- a/DeepRL-Agents
+++ b/DeepRL-Agents
@@ -1 +1 @@
-Subproject commit 1bec6b59b94186591e4f430f268ad1b2bd80130e
+Subproject commit 1bec6b59b94186591e4f430f268ad1b2bd80130e-dirty
diff --git a/TRPO/main_di.py b/TRPO/main_di.py
index 92dbcc5..14774cf 100644
--- a/TRPO/main_di.py
+++ b/TRPO/main_di.py
@@ -18,7 +18,7 @@ import tensorflow.contrib.layers as layers
 #   Training Parameters
 # =========================
 # Simulation step
-PLOT = False
+PLOT = True
 
 SIM_TIME_STEP = 0.1
 
diff --git a/TRPO/quad/a.diff b/TRPO/quad/a.diff
index ff1290c..2b3e469 100644
--- a/TRPO/quad/a.diff
+++ b/TRPO/quad/a.diff
@@ -1,3122 +0,0 @@
-diff --git a/.DS_Store b/.DS_Store
-index 1976ffd..1930f87 100644
-Binary files a/.DS_Store and b/.DS_Store differ
-diff --git a/A3C/simulator.py b/A3C/simulator.py
-index 105ee78..4e30424 100644
---- a/A3C/simulator.py
-+++ b/A3C/simulator.py
-@@ -1,312 +1,332 @@
--#!/usr/bin/env python
--# -*- coding: utf-8 -*-
--# File: simulator.py
--# Author: Yuxin Wu <ppwwyyxxc@gmail.com>
--
--import tensorflow as tf
--import multiprocessing as mp
--import time
--import os
--import threading
--from abc import abstractmethod, ABCMeta
--from collections import defaultdict
--
--import six
--from six.moves import queue
--import zmq
--
--from tensorpack.models.common import disable_layer_logging
--from tensorpack.callbacks import Callback
--from tensorpack.tfutils.varmanip import SessionUpdate
--from tensorpack.predict import OfflinePredictor
--from tensorpack.utils import logger
--from tensorpack.utils.serialize import loads, dumps
--from tensorpack.utils.concurrency import LoopThread, ensure_proc_terminate
--
--__all__ = ['SimulatorProcess', 'SimulatorMaster',
--           'SimulatorProcessStateExchange',
--           'TransitionExperience']
--
--
--class TransitionExperience(object):
--    """ A transition of state, or experience"""
--
--    def __init__(self, state, action, reward, **kwargs):
--        """ kwargs: whatever other attribute you want to save"""
--        self.state = state
--        self.action = action
--        self.reward = reward
--        for k, v in six.iteritems(kwargs):
--            setattr(self, k, v)
--
--
--@six.add_metaclass(ABCMeta)
--class SimulatorProcessBase(mp.Process):
--    def __init__(self, idx):
--        super(SimulatorProcessBase, self).__init__()
--        self.idx = int(idx)
--        self.name = u'simulator-{}'.format(self.idx)
--        self.identity = self.name.encode('utf-8')
--
--    @abstractmethod
--    def _build_player(self):
--        pass
--
--
--class SimulatorProcessStateExchange(SimulatorProcessBase):
--    """
--    A process that simulates a player and communicates to master to
--    send states and receive the next action
--    """
--
--    def __init__(self, idx, pipe_c2s, pipe_s2c):
--        """
--        :param idx: idx of this process
--        """
--        super(SimulatorProcessStateExchange, self).__init__(idx)
--        self.c2s = pipe_c2s
--        self.s2c = pipe_s2c
--
--    def run(self):
--        player = self._build_player()
--        context = zmq.Context()
--        c2s_socket = context.socket(zmq.PUSH)
--        c2s_socket.setsockopt(zmq.IDENTITY, self.identity)
--        c2s_socket.set_hwm(2)
--        c2s_socket.connect(self.c2s)
--
--        s2c_socket = context.socket(zmq.DEALER)
--        s2c_socket.setsockopt(zmq.IDENTITY, self.identity)
--        # s2c_socket.set_hwm(5)
--        s2c_socket.connect(self.s2c)
--
--        state = player.current_state()
--        reward, isOver = 0, False
--        while True:
--            c2s_socket.send(dumps(
--                (self.identity, state, reward, isOver)),
--                copy=False)
--            action = loads(s2c_socket.recv(copy=False).bytes)
--            reward, isOver = player.action(action)
--            state = player.current_state()
--
--
--# compatibility
--SimulatorProcess = SimulatorProcessStateExchange
--
--
--class SimulatorMaster(threading.Thread):
--    """ A base thread to communicate with all StateExchangeSimulatorProcess.
--        It should produce action for each simulator, as well as
--        defining callbacks when a transition or an episode is finished.
--    """
--    class ClientState(object):
--        def __init__(self):
--            self.memory = []    # list of Experience
--
--    def __init__(self, pipe_c2s, pipe_s2c):
--        super(SimulatorMaster, self).__init__()
--        assert os.name != 'nt', "Doesn't support windows!"
--        self.daemon = True
--        self.name = 'SimulatorMaster'
--
--        self.context = zmq.Context()
--
--        self.c2s_socket = self.context.socket(zmq.PULL)
--        self.c2s_socket.bind(pipe_c2s)
--        self.c2s_socket.set_hwm(10)
--        self.s2c_socket = self.context.socket(zmq.ROUTER)
--        self.s2c_socket.bind(pipe_s2c)
--        self.s2c_socket.set_hwm(10)
--
--        # queueing messages to client
--        self.send_queue = queue.Queue(maxsize=100)
--
--        def f():
--            msg = self.send_queue.get()
--            self.s2c_socket.send_multipart(msg, copy=False)
--        self.send_thread = LoopThread(f)
--        self.send_thread.daemon = True
--        self.send_thread.start()
--
--        # make sure socket get closed at the end
--        def clean_context(soks, context):
--            for s in soks:
--                s.close()
--            context.term()
--        import atexit
--        atexit.register(clean_context, [self.c2s_socket, self.s2c_socket], self.context)
--
--    def run(self):
--        self.clients = defaultdict(self.ClientState)
--        try:
--            while True:
--                msg = loads(self.c2s_socket.recv(copy=False).bytes)
--                ident, state, reward, isOver = msg
--                # TODO check history and warn about dead client
--                client = self.clients[ident]
--
--                # check if reward&isOver is valid
--                # in the first message, only state is valid
--                if len(client.memory) > 0:
--                    client.memory[-1].reward = reward
--                    if isOver:
--                        self._on_episode_over(ident)
--                    else:
--                        self._on_datapoint(ident)
--                # feed state and return action
--                self._on_state(state, ident)
--        except zmq.ContextTerminated:
--            logger.info("[Simulator] Context was terminated.")
--
--    @abstractmethod
--    def _on_state(self, state, ident):
--        """response to state sent by ident. Preferrably an async call"""
--
--    @abstractmethod
--    def _on_episode_over(self, client):
--        """ callback when the client just finished an episode.
--            You may want to clear the client's memory in this callback.
--        """
--
--    def _on_datapoint(self, client):
--        """ callback when the client just finished a transition
--        """
--
--    def __del__(self):
--        self.context.destroy(linger=0)
--
--
--# ------------------- the following code are not used at all. Just experimental
--class SimulatorProcessDF(SimulatorProcessBase):
--    """ A simulator which contains a forward model itself, allowing
--    it to produce data points directly """
--
--    def __init__(self, idx, pipe_c2s):
--        super(SimulatorProcessDF, self).__init__(idx)
--        self.pipe_c2s = pipe_c2s
--
--    def run(self):
--        self.player = self._build_player()
--
--        self.ctx = zmq.Context()
--        self.c2s_socket = self.ctx.socket(zmq.PUSH)
--        self.c2s_socket.setsockopt(zmq.IDENTITY, self.identity)
--        self.c2s_socket.set_hwm(5)
--        self.c2s_socket.connect(self.pipe_c2s)
--
--        self._prepare()
--        for dp in self.get_data():
--            self.c2s_socket.send(dumps(dp), copy=False)
--
--    @abstractmethod
--    def _prepare(self):
--        pass
--
--    @abstractmethod
--    def get_data(self):
--        pass
--
--
--class SimulatorProcessSharedWeight(SimulatorProcessDF):
--    """ A simulator process with an extra thread waiting for event,
--    and take shared weight from shm.
--
--    Start me under some CUDA_VISIBLE_DEVICES set!
--    """
--
--    def __init__(self, idx, pipe_c2s, condvar, shared_dic, pred_config):
--        super(SimulatorProcessSharedWeight, self).__init__(idx, pipe_c2s)
--        self.condvar = condvar
--        self.shared_dic = shared_dic
--        self.pred_config = pred_config
--
--    def _prepare(self):
--        disable_layer_logging()
--        self.predictor = OfflinePredictor(self.pred_config)
--        with self.predictor.graph.as_default():
--            vars_to_update = self._params_to_update()
--            self.sess_updater = SessionUpdate(
--                self.predictor.session, vars_to_update)
--        # TODO setup callback for explore?
--        self.predictor.graph.finalize()
--
--        self.weight_lock = threading.Lock()
--
--        # start a thread to wait for notification
--        def func():
--            self.condvar.acquire()
--            while True:
--                self.condvar.wait()
--                self._trigger_evt()
--        self.evt_th = threading.Thread(target=func)
--        self.evt_th.daemon = True
--        self.evt_th.start()
--
--    def _trigger_evt(self):
--        with self.weight_lock:
--            self.sess_updater.update(self.shared_dic['params'])
--            logger.info("Updated.")
--
--    def _params_to_update(self):
--        # can be overwritten to update more params
--        return tf.trainable_variables()
--
--
--class WeightSync(Callback):
--    """ Sync weight from main process to shared_dic and notify"""
--
--    def __init__(self, condvar, shared_dic):
--        self.condvar = condvar
--        self.shared_dic = shared_dic
--
--    def _setup_graph(self):
--        self.vars = self._params_to_update()
--
--    def _params_to_update(self):
--        # can be overwritten to update more params
--        return tf.trainable_variables()
--
--    def _before_train(self):
--        self._sync()
--
--    def _trigger_epoch(self):
--        self._sync()
--
--    def _sync(self):
--        logger.info("Updating weights ...")
--        dic = {v.name: v.eval() for v in self.vars}
--        self.shared_dic['params'] = dic
--        self.condvar.acquire()
--        self.condvar.notify_all()
--        self.condvar.release()
--
--
--if __name__ == '__main__':
--    import random
--    from tensorpack.RL import NaiveRLEnvironment
--
--    class NaiveSimulator(SimulatorProcess):
--
--        def _build_player(self):
--            return NaiveRLEnvironment()
--
--    class NaiveActioner(SimulatorMaster):
--        def _get_action(self, state):
--            time.sleep(1)
--            return random.randint(1, 12)
--
--        def _on_episode_over(self, client):
--            # print("Over: ", client.memory)
--            client.memory = []
--            client.state = 0
--
--    name = 'ipc://whatever'
--    procs = [NaiveSimulator(k, name) for k in range(10)]
--    [k.start() for k in procs]
--
--    th = NaiveActioner(name)
--    ensure_proc_terminate(procs)
--    th.start()
--
--    time.sleep(100)
-+#!/usr/bin/python2.7
-+# Filename: simulator.py
-+# Description: This is the simulator of quadcopter with an inverted pendulum on it
-+# Auther: Peng Wei, Zhenghao Fei
-+
-+
-+import numpy as np
-+from scipy.integrate import odeint
-+
-+class QuadCopter(object):
-+    def __init__(self, Ts=0.01, max_time = 10, inverted_pendulum=True):
-+    # simulator  step time
-+        self.Ts          = Ts
-+        self.max_time = max_time
-+        self.stateSpace  = 16
-+        self.actionSpace = 4
-+        self.actionLimit  = 5.0 # maximum rotor speed degree/s TBD
-+        self.inverted_pendulum = inverted_pendulum
-+
-+    # physical parameters of airframe
-+        # self.gravity = 9.81
-+        # self.l       = 0.175  # m, Distance between rotor and center
-+        # self.pen_l   = 0.20  # m, the length of stick
-+        # self.k1      = 1.0      # propellers constant
-+        # self.k2      = 2.0      # propellers constant 
-+        # self.mass    = 0.5  # mass
-+        # self.Jx      = 2.32e-3
-+        # self.Jy      = 2.32e-3
-+        # self.Jz      = 4.00e-3
-+
-+        self.gravity = 9.81
-+        self.l = 0.2; # m, Distance between rotor and center
-+        self.pen_l   = 0.20  # m, the length of stick
-+        self.k1 = 100; # propellers constant
-+        self.k2 = 100; # propellers constant 
-+        self.R = 0.04; # m, Center mass radius 
-+        self.M = 1 # kg, Body weight
-+        self.m = 0.07 #kg, Rotor weight 
-+        self.mass = self.M + self.m;
-+        self.Jx   = 2*self.M*self.R**2/5 + 2*self.l*self.m;
-+        self.Jy   = 2*self.M*self.R**2/5 + 2*self.l*self.m;
-+        self.Jz   = 2*self.M*self.R**2/5 + 4*self.l*self.m;
-+
-+    # initial conditions
-+        self.pn0    = 0.0  # initial North position
-+        self.pe0    = 0.0  # initial East position
-+        self.pd0    = 0.0  # initial Down position (negative altitude)
-+        self.u0     = 0.0  # initial velocity along body x-axis
-+        self.v0     = 0.0  # initial velocity along body y-axis
-+        self.w0     = 0.0  # initial velocity along body z-axis
-+        self.phi0   = 0.0  # initial roll angle
-+        self.theta0 = 0.0  # initial pitch angle
-+        self.psi0   = 0.0  # initial yaw angle
-+        self.p0     = 0.0  # initial body frame roll rate
-+        self.q0     = 0.0  # initial body frame pitch rate
-+        self.r0     = 0.0  # initial body frame yaw rate
-+
-+    # initial conditions for inverted pendulum
-+        self.pen_x0    = 0.0 # initial displacement along iv in vehicle frame
-+        self.pen_y0    = 0.0 # initial displacement along jv in vehicle frame
-+        self.pen_vx0   = 0.0 # initial velocity along iv in vehicle frame
-+        self.pen_vy0   = 0.0 # initial velocity along jv in vehicle frame
-+
-+    # maximum conditions
-+        self.pn_max    =  100  # max North position
-+        self.pe_max    =  100  # max East position
-+        self.pd_max    =  100  # max Down position (negative altitude)
-+        self.u_max     = 10 # max velocity along body x-axis
-+        self.v_max     = 10 # max velocity along body y-axis
-+        self.w_max     = 10 # max velocity along body z-axis
-+        self.p_max     = 10 # max body frame roll rate
-+        self.q_max     = 10 # max body frame pitch rate
-+        self.r_max     = 10 # max body frame yaw rate
-+
-+    # apply initial conditions
-+        self.reset()
-+
-+    def reset(self):
-+        # print "system reset"
-+        self.pn     = self.pn0
-+        self.pe     = self.pe0
-+        self.pd     = self.pd0
-+        self.u      = self.u0
-+        self.v      = self.v0
-+        self.w      = self.w0 
-+        self.phi    = self.phi0
-+        self.theta  = self.theta0
-+        self.psi    = self.psi0
-+        self.p      = self.p0
-+        self.q      = self.q0
-+        self.r      = self.r0
-+        self.pen_x  = self.pen_x0
-+        self.pen_y  = self.pen_y0
-+        self.pen_vx = self.pen_vx0
-+        self.pen_vy = self.pen_vy0
-+        self.time   = 0.0 # simulation time
-+
-+        self.states = np.asarray([self.pn, self.pe, self.pd, self.u, self.v, self.w, self.phi, self.theta, self.psi,
-+                                  self.p,  self.q,  self.r,  self.pen_x,  self.pen_y,  self.pen_vx,  self.pen_vy])
-+        return self.states
-+
-+    def force(self, x):
-+        f = self.k1 * x
-+        return f 
-+
-+    def torque(self, x):
-+        tau = self.k2 * x
-+        return tau
-+
-+    def trunc_error(self,x):
-+        if np.absolute(x) < 1e-15:
-+            return 0.0
-+        else:
-+            return x
-+
-+    def forces_moments(self, delta_f, delta_r, delta_b, delta_l, theta, phi):
-+        Force_x = -self.mass * self.gravity * np.sin(theta);
-+        Force_y =  self.mass * self.gravity * np.cos(theta) * np.sin(phi)
-+        Force_z =  self.mass * self.gravity * np.cos(theta) * np.cos(phi) \
-+                - (self.force(delta_f)+self.force(delta_r)+self.force(delta_b)+self.force(delta_l))
-+
-+        Torque_x = -self.l * self.force(delta_r) + self.l * self.force(delta_l)     
-+        Torque_y = self.l * self.force(delta_f) - self.l * self.force(delta_b)
-+        Torque_z = -self.torque(delta_f) + self.torque(delta_r) - self.torque(delta_b) + self.torque(delta_l)
-+
-+        uu = np.asarray([self.trunc_error(Force_x), self.trunc_error(Force_y), self.trunc_error(Force_z),
-+                         self.trunc_error(Torque_x), self.trunc_error(Torque_y), self.trunc_error(Torque_z)])
-+
-+        return uu
-+
-+
-+
-+    def Derivative(self, states, t, delta_f, delta_r, delta_b, delta_l):
-+    # state variables
-+        pn     = states[0]    
-+        pe     = states[1]    
-+        pd     = states[2]   
-+        u      = states[3]     
-+        v      = states[4]    
-+        w      = states[5]   
-+        phi    = states[6] 
-+        theta  = states[7] 
-+        psi    = states[8]  
-+        p      = states[9]    
-+        q      = states[10]   
-+        r      = states[11]    
-+        pen_x  = states[12] 
-+        pen_y  = states[13] 
-+        pen_vx = states[14] 
-+        pen_vy = states[15] 
-+    # control inputs
-+        uu    = self.forces_moments(delta_f, delta_r, delta_b, delta_l, theta, phi)
-+        fx    = uu[0]
-+        fy    = uu[1]
-+        fz    = uu[2]
-+        taup  = uu[3] #tau phi
-+        taut  = uu[4] #tau theta
-+        taus  = uu[5] #tau psi
-+    
-+        sp = np.sin(phi)
-+        cp = np.cos(phi)
-+        st = np.sin(theta)
-+        ct = np.cos(theta)
-+        ss = np.sin(psi)
-+        cs = np.cos(psi)
-+        tt = np.tan(theta)
-+
-+     # translational kinematics
-+        rotation_position = np.mat([[ct*cs, sp*st*cs-cp*ss, cp*st*cs+sp*ss], 
-+                                    [ct*ss, sp*st*ss+cp*cs, cp*st*ss-sp*cs],
-+                                    [-st,   sp*ct,          cp*ct]])
-+                                   
-+        position_dot = rotation_position * np.mat([u,v,w]).T
-+        pndot = position_dot[0,0]
-+        pedot = position_dot[1,0]
-+        pddot = position_dot[2,0]
-+    
-+     # translational dynamics
-+        udot = r*v - q*w + fx/self.mass
-+        vdot = p*w - r*u + fy/self.mass
-+        wdot = q*u - p*v + fz/self.mass
-+    
-+     # rotational kinematics
-+        rotation_angle = np.mat([[1, sp*tt, cp*tt],
-+                                 [0, cp,    -sp],
-+                                 [0, sp/ct, cp/ct]])
-+                      
-+        angle_dot = rotation_angle * np.mat([p, q, r]).T
-+        phidot    = angle_dot[0,0]
-+        thetadot  = angle_dot[1,0]
-+        psidot    = angle_dot[2,0]
-+
-+     # rotational dynamics
-+        pdot = (q*r*(self.Jy-self.Jz)/self.Jx) + (taup/self.Jx)
-+        qdot = (p*r*(self.Jz-self.Jx)/self.Jy) + (taut/self.Jy)
-+        rdot = (p*q*(self.Jx-self.Jy)/self.Jz) + (taus/self.Jz)
-+
-+     # inverted pendulum kinematics
-+        pen_accel = rotation_position * np.mat([udot,vdot,wdot]).T
-+        xddot     = pen_accel[0,0]
-+        yddot     = pen_accel[1,0]
-+        zddot     = pen_accel[2,0]
-+
-+     # inverted pendulum dynamics
-+        if self.inverted_pendulum: 
-+            pen_zeta  = np.sqrt(self.pen_l**2.0 - pen_x**2.0 - pen_y**2.0)
-+            pen_xdot  = pen_vx
-+            pen_ydot  = pen_vy
-+            pen_alpha = (-pen_zeta**2.0/(pen_zeta**2.0+pen_x**2.0)) * (xddot+(pen_xdot**2.0*pen_x+pen_ydot**2.0*pen_x)/(pen_zeta**2.0) \
-+                      + (pen_xdot**2.0*pen_x**3.0+2*pen_xdot*pen_ydot*pen_x**2.0*pen_y+pen_ydot**2.0*pen_y**2.0*pen_x)/(pen_zeta**4.0) \
-+                      - (pen_x*(zddot+self.gravity))/(pen_zeta))
-+            pen_beta  = (-pen_zeta**2.0/(pen_zeta**2.0+pen_y**2.0)) * (yddot+(pen_ydot**2.0*pen_y+pen_xdot**2.0*pen_y)/(pen_zeta**2.0) \
-+                      + (pen_ydot**2.0*pen_y**3.0+2*pen_ydot*pen_xdot*pen_y**2.0*pen_x+pen_xdot**2.0*pen_x**2.0*pen_y)/(pen_zeta**4.0) \
-+                      - (pen_y*(zddot+self.gravity))/(pen_zeta))
-+            pen_vxdot = (pen_alpha - pen_beta*pen_x*pen_y/((self.pen_l**2.0-pen_y**2.0)*pen_zeta**2.0)) \
-+                      * (1.0 - (pen_x**2.0*pen_y**2.0)/((self.pen_l**2.0-pen_y**2.0)**2.0*pen_zeta**4.0))
-+            pen_vydot = pen_beta - (pen_vxdot*pen_x*pen_y)/((self.pen_l**2.0-pen_x**2.0)*pen_zeta**2.0)
-+        else:
-+            pen_zeta  = 0
-+            pen_xdot  = 0
-+            pen_ydot  = 0
-+            pen_alpha = 0
-+            pen_beta  = 0
-+            pen_vxdot = 0
-+            pen_vydot = 0
-+
-+        states_dot = np.asarray([pndot, pedot,  pddot,  udot,   vdot,   wdot,   phidot, thetadot,   psidot, pdot,   qdot,   rdot,
-+                                 pen_xdot,  pen_ydot,   pen_vxdot,  pen_vydot])
-+        return states_dot
-+
-+    def naive_int(self, derivative_func, states, Ts, args):
-+        states_dot = derivative_func(states, Ts, args[0], args[1], args[2], args[3])
-+        states += states_dot*Ts
-+        sol =  np.vstack((states_dot, states))
-+        return sol
-+
-+
-+    def step(self, delta):
-+        terminated = False
-+        info = 'normal'
-+
-+        delta   = np.asarray(delta)*3.1416/180
-+        delta_f = delta[0]
-+        delta_r = delta[1]
-+        delta_b = delta[2]
-+        delta_l = delta[3]
-+
-+    # integral, ode
-+        # sol = odeint(self.Derivative, self.states, [self.time, self.time+self.Ts], args=(delta_f,delta_r,delta_b,delta_l), full_output=False, printmessg=False)
-+        sol = self.naive_int(self.Derivative, self.states, self.Ts, [delta_f,delta_r,delta_b,delta_l])
-+
-+        self.pn     = sol[1,0] 
-+        self.pe     = sol[1,1] 
-+        self.pd     = sol[1,2] 
-+        self.u      = sol[1,3]
-+        self.v      = sol[1,4]   
-+        self.w      = sol[1,5]
-+        self.phi    = sol[1,6]
-+        self.theta  = sol[1,7]
-+        self.psi    = sol[1,8]
-+        self.p      = sol[1,9]  
-+        self.q      = sol[1,10]  
-+        self.r      = sol[1,11]  
-+        self.pen_x  = sol[1,12]
-+        self.pen_y  = sol[1,13]
-+        self.pen_vx = sol[1,14]
-+        self.pen_vy = sol[1,15]
-+        self.time   += self.Ts
-+
-+        # check boundry condition
-+        if self.pn > self.pn_max:
-+            self.pn = self.pn_max
-+        if self.pn < -self.pn_max:
-+            self.pn = -self.pn_max
-+
-+        if self.pe > self.pe_max:
-+            self.pe = self.pe_max
-+        if self.pe < -self.pe_max:
-+            self.pe = -self.pe_max
-+
-+        if self.pd > self.pd_max:
-+            self.pd = self.pd_max
-+        if self.pd < -self.pd_max:
-+            self.pd = -self.pd_max
-+
-+        if self.u > self.u_max:
-+            self.u = self.u_max
-+        if self.u < -self.u_max:
-+            self.u = -self.u_max
-+
-+        if self.v > self.v_max:
-+            self.v = self.v_max
-+        if self.v < -self.v_max:
-+            self.v = -self.v_max
-+
-+        if self.w > self.w_max:
-+            self.w = self.w_max
-+        if self.w < -self.w_max:
-+            self.w = -self.w_max
-+
-+        if self.p > self.p_max:
-+            self.p = self.p_max
-+        if self.p < -self.p_max:
-+            self.p = -self.p_max
-+
-+        if self.q > self.q_max:
-+            self.q = self.q_max
-+        if self.q < -self.q_max:
-+            self.q = -self.q_max
-+
-+        if self.r > self.r_max:
-+            self.r = self.r_max
-+        if self.r < -self.r_max:
-+            self.r = -self.r_max
-+
-+        if self.time > self.max_time:
-+            terminated = True
-+            info = 'timeout'   
-+    # # Fail condition check
-+    #     if self.pd > 0:
-+    #         terminated = True
-+    #         info = 'crash'   
-+
-+        # print 'Time = %f' %self.time
-+        self.states = np.asarray([self.pn, self.pe, self.pd, self.u, self.v, self.w, self.phi, self.theta, self.psi,
-+                                  self.p,  self.q,  self.r,  self.pen_x,  self.pen_y,  self.pen_vx,  self.pen_vy])
-+        if  terminated:
-+            self.reset()
-+
-+        return self.states, terminated, info
-+
-+# end class
-\ No newline at end of file
-diff --git a/A3C/train-quad.py b/A3C/train-quad.py
-index a785829..f8acf4b 100755
---- a/A3C/train-quad.py
-+++ b/A3C/train-quad.py
-@@ -47,7 +47,7 @@ PREDICTOR_THREAD_PER_GPU = 2
- PREDICTOR_THREAD = None
- EVALUATE_PROC = min(multiprocessing.cpu_count() // 2, 20)
- 
--NUM_ACTIONS = None
-+NUM_ACTIONS = 3
- ENV_NAME = None
- 
- 
-@@ -235,15 +235,15 @@ if __name__ == '__main__':
-     parser = argparse.ArgumentParser()
-     parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.')
-     parser.add_argument('--load', help='load model')
--    parser.add_argument('--env', help='env', required=True)
-+    # parser.add_argument('--env', help='env', required=True)
-     parser.add_argument('--task', help='task to perform',
-                         choices=['play', 'eval', 'train'], default='train')
-     args = parser.parse_args()
- 
--    ENV_NAME = args.env
--    assert ENV_NAME
--    p = get_player()
--    del p    # set NUM_ACTIONS
-+    # ENV_NAME = args.env
-+    # assert ENV_NAME
-+    # p = get_player()
-+    # del p    # set NUM_ACTIONS
- 
-     if args.gpu:
-         os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
-diff --git a/TRPO/main_dl.py b/TRPO/main_dl.py
-deleted file mode 100644
-index 57a4763..0000000
---- a/TRPO/main_dl.py
-+++ /dev/null
-@@ -1,304 +0,0 @@
--import numpy as np
--from simulator_di import QuadCopter
--from  util import *
--from quad_task import *
--
--import tensorflow as tf
--import tensorflow.contrib.layers as layers
--import gym
--import logz
--import scipy.signal
--# ==========================
--#   Training Parameters
--# =========================
--# Simulation step
--SIM_TIME_STEP = 0.01
--
--# Max episode length
--MAX_EP_TIME = 2 # second
--MAX_EP_STEPS = int(MAX_EP_TIME/SIM_TIME_STEP)
--
--
--def normc_initializer(std=1.0):
--    """
--    Initialize array with normalized columns
--    """
--    def _initializer(shape, dtype=None, partition_info=None): #pylint: disable=W0613
--        out = np.random.randn(*shape).astype(np.float32)
--        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))
--        return tf.constant(out)
--    return _initializer
--
--
--def dense(x, size, name, weight_init=None):
--    """
--    Dense (fully connected) layer
--    """
--    w = tf.get_variable(name + "/w", [x.get_shape()[1], size], initializer=weight_init)
--    b = tf.get_variable(name + "/b", [size], initializer=tf.zeros_initializer())
--    return tf.matmul(x, w) + b
--
--def fancy_slice_2d(X, inds0, inds1):
--    """
--    Like numpy's X[inds0, inds1]
--    """
--    inds0 = tf.cast(inds0, tf.int64)
--    inds1 = tf.cast(inds1, tf.int64)
--    shape = tf.cast(tf.shape(X), tf.int64)
--    ncols = shape[1]
--    Xflat = tf.reshape(X, [-1])
--    return tf.gather(Xflat, inds0 * ncols + inds1)
--
--def discount(x, gamma):
--    """
--    Compute discounted sum of future values
--    out[i] = in[i] + gamma * in[i+1] + gamma^2 * in[i+2] + ...
--    """
--    return scipy.signal.lfilter([1],[1,-gamma],x[::-1], axis=0)[::-1]
--
--def explained_variance_1d(ypred,y):
--    """
--    Var[ypred - y] / var[y]. 
--    https://www.quora.com/What-is-the-meaning-proportion-of-variance-explained-in-linear-regression
--    """
--    assert y.ndim == 1 and ypred.ndim == 1    
--    vary = np.var(y)
--    return np.nan if vary==0 else 1 - np.var(y-ypred)/vary
--
--def categorical_sample_logits(logits):
--    """
--    Samples (symbolically) from categorical distribution, where logits is a NxK
--    matrix specifying N categorical distributions with K categories
--
--    specifically, exp(logits) / sum( exp(logits), axis=1 ) is the 
--    probabilities of the different classes
--
--    Cleverly uses gumbell trick, based on
--    https://github.com/tensorflow/tensorflow/issues/456
--    """
--    U = tf.random_uniform(tf.shape(logits))
--    return tf.argmax(logits - tf.log(-tf.log(U)), dimension=1)
--
--def sample_gaussian(ac_dim, mean, std):
--    """
--    sample from gaussian
--    """
--    sample = tf.random_normal([ac_dim], mean=mean, stddev=std, dtype=tf.float32, seed=None, name=None)
--
--    return sample
--
--
--def pathlength(path):
--    return len(path["reward"])
--
--class LinearValueFunction(object):
--    coef = None
--    def fit(self, X, y):
--        Xp = self.preproc(X)
--        A = Xp.T.dot(Xp)
--        nfeats = Xp.shape[1]
--        A[np.arange(nfeats), np.arange(nfeats)] += 1e-3 # a little ridge regression
--        b = Xp.T.dot(y)
--        self.coef = np.linalg.solve(A, b)
--    def predict(self, X):
--        if self.coef is None:
--            return np.zeros(X.shape[0])
--        else:
--            return self.preproc(X).dot(self.coef)
--    def preproc(self, X):
--        return np.concatenate([np.ones([X.shape[0], 1]), X, np.square(X)/2.0], axis=1)
--
--
--def main_pendulum(logdir, seed, n_iter, gamma, min_timesteps_per_batch, initial_stepsize, desired_kl, vf_type, vf_params, animate=False):
--    tf.set_random_seed(seed)
--    np.random.seed(seed)
--    env  = QuadCopter(SIM_TIME_STEP, inverted_pendulum=False)
--    ob_dim = env.stateSpace
--    ac_dim = env.actionSpace
--    ac_lim = env.actionLimit
--    print("Quadcopter created")
--    print('state_dim: ', ob_dim)
--    print('action_dim: ', ac_dim)
--    print('action_limit: ',ac_lim)
--    print('max time: ', MAX_EP_TIME)
--    print('max step: ',MAX_EP_STEPS)        
--
--    hover_position = np.asarray([0, 0, -10])
--    task = hover(hover_position)
--
--    logz.configure_output_dir(logdir)
--    if vf_type == 'linear':
--        vf = LinearValueFunction(**vf_params)
--    elif vf_type == 'nn':
--        vf = NnValueFunction(ob_dim=ob_dim, **vf_params)
--
--
--
--    # Symbolic variables have the prefix sy_, to distinguish them from the numerical values
--    # that are computed later in these function
--    sy_ob_no = tf.placeholder(shape=[None, ob_dim], name="ob", dtype=tf.float32) # batch of observations
--    sy_ac_n = tf.placeholder(shape=[None, ac_dim], name="ac", dtype=tf.float32) # batch of actions taken by the policy, used for policy gradient computation
--    sy_adv_n = tf.placeholder(shape=[None, 1], name="adv", dtype=tf.float32) # advantage function estimate
--    sy_h1 = tf.nn.relu(dense(sy_ob_no, 400, "h1", weight_init=normc_initializer(1.0))) # hidden layer
--    sy_h2 = tf.nn.relu(dense(sy_h1, 300, "h2", weight_init=normc_initializer(1.0))) # hidden layer
--
--    # mean_na = dense(sy_h1, ac_dim, "mean", weight_init=normc_initializer(0.05)) # "logits", describing probability distribution of final layer
--    
--    mean_na = tf.tanh(dense(sy_h2, ac_dim, "final",weight_init=normc_initializer(0.1)))*ac_lim # Mean control output
--    # std_a = tf.constant(1.0, dtype=tf.float32, shape=[ac_dim])
--    std_a =  tf.get_variable("logstdev", [ac_dim], initializer=tf.ones_initializer())
--
--    # std_a = tf.constant(1.0,  shape=[ac_dim], dtype=tf.float32)
--
--    sy_sampled_ac = sample_gaussian(ac_dim, mean_na, std_a) # sampled actions, used for defining the policy (NOT computing the policy gradient)
--    # sy_sampled_ac = tf.zeros([1, ac_dim])
--    sy_prob_n = (1.0/tf.sqrt((tf.square(std_a)*2*3.1415926))) * tf.exp(-0.5*tf.square((sy_ac_n - mean_na)/std_a))
--    # sy_prob_n = (1.0/(std_a*2.5067)) * tf.exp(-0.5*tf.square((sy_ac_n - mean_na)/std_a))
--
--    sy_logprob_n = tf.log(sy_prob_n)
--    # sub = tf.subtract(sy_ac_n, mean_na)
--    # mul = tf.multiply(sub, sy_h1)
--    # sy_logprob_n = tf.log(tf.divide(sub, tf.square(std_a))) # log-prob of actions taken -- used for policy gradient calculation
--
--    # The following quantities are just used for computing KL and entropy, JUST FOR DIAGNOSTIC PURPOSES >>>>
--    sy_n = tf.shape(sy_ob_no)[0]
--    old_mean_na = tf.placeholder(shape=[None, ac_dim], name='old_mean_a', dtype=tf.float32) # mean_a BEFORE update (just used for KL diagnostic)
--    old_std_a = tf.placeholder(shape=[ac_dim], name='old_std_a', dtype=tf.float32) # std_a BEFORE update (just used for KL diagnostic)
--    # KL 
--    sy_kl = tf.reduce_mean(tf.log(std_a/old_std_a) + (tf.square(old_std_a) + tf.square(old_mean_na - mean_na))/(2*tf.square(std_a)) - 0.5)
--    # entropy
--    sy_p_na = tf.exp(mean_na)
--    sy_ent = tf.reduce_sum( - sy_p_na * mean_na) / tf.to_float(sy_n)
--    # <<<<<<<<<<<<<
--
--    sy_surr = - tf.reduce_mean(sy_adv_n * sy_logprob_n) # Loss function that we'll differentiate to get the policy gradient ("surr" is for "surrogate loss")
--
--    sy_stepsize = tf.placeholder(shape=[], dtype=tf.float32) # Symbolic, in case you want to change the stepsize during optimization. (We're not doing that currently)
--    update_op = tf.train.AdamOptimizer(sy_stepsize).minimize(sy_surr)
--
--    sess = tf.Session()
--    sess.__enter__() # equivalent to `with sess:`
--    tf.global_variables_initializer().run() #pylint: disable=E1101
--
--    total_timesteps = 0
--    stepsize = initial_stepsize
--    for i in range(n_iter):
--
--        print("********** Iteration %i ************"%i)
--
--        # Collect paths until we have enough timesteps
--
--        timesteps_this_batch = 0
--        paths = []
--        while True:
--            ob = env.reset()
--            ob_last = np.copy(ob)
--            terminated = False
--            obs, acs, rewards = [], [], []
--            j = 0
--            while True:
--                j += 1
--                ob = ob.reshape(ob.shape[0],)
--                obs.append(ob)
--                # print ob
--                # mean = sess.run(mean_na, feed_dict={sy_ob_no : ob[None]})[0]
--                ac = sess.run(sy_sampled_ac, feed_dict={sy_ob_no : ob[None]})[0]
--                # print ac
--                ob, done, _ = env.step(ac)
--                rew = task.reward(ob, done, _)
--                # ac = np.asscalar(ac)
--                acs.append(ac)
--
--                rew = np.asscalar(rew)
--                rewards.append(rew)
--                if done:
--                    # print "done"
--                    print ob_last[0:3]
--                    break       
--                ob_last = np.copy(ob)
--             
--            path = {"observation" : np.array(obs), "terminated" : terminated,
--                    "reward" : np.array(rewards), "action" : np.array(acs)}
--            paths.append(path)
--            timesteps_this_batch += pathlength(path)
--            if timesteps_this_batch > min_timesteps_per_batch:
--                break
--        total_timesteps += timesteps_this_batch
--        # Estimate advantage function
--        vtargs, vpreds, advs = [], [], []
--        for path in paths:
--            rew_t = path["reward"]
--            return_t = discount(rew_t, gamma)
--            vpred_t = vf.predict(path["observation"])
--            adv_t = return_t - vpred_t
--            # print("return_t: ", return_t.shape)
--            # print("vpred_t: ", vpred_t.shape)
--            # print("adv_t: ", adv_t.shape)
--
--            advs.append(adv_t)
--            vtargs.append(return_t)
--            vpreds.append(vpred_t)
--
--
--        # Build arrays for policy update
--        ob_no = np.concatenate([path["observation"] for path in paths])
--        ac_n = np.concatenate([path["action"] for path in paths])
--        ac_n = ac_n.reshape([-1, ac_dim])
--        adv_n = np.concatenate(advs)
--        standardized_adv_n = (adv_n - adv_n.mean()) / (adv_n.std() + 1e-8)
--        standardized_adv_n = standardized_adv_n.reshape([-1, 1])
--
--        vtarg_n = np.concatenate(vtargs)
--        vpred_n = np.concatenate(vpreds)
--        vf.fit(ob_no, vtarg_n)
--
--        # Policy update
--        # print standardized_adv_n
--        surr, adv, logp = sess.run([sy_surr, sy_adv_n, sy_prob_n], feed_dict={sy_ob_no:ob_no, sy_ac_n:ac_n, sy_adv_n:standardized_adv_n, sy_stepsize:stepsize})
--        _, old_mean, old_std = sess.run([update_op, mean_na, std_a], feed_dict={sy_ob_no:ob_no, sy_ac_n:ac_n, sy_adv_n:standardized_adv_n, sy_stepsize:stepsize})
--        kl, ent = sess.run([sy_kl, sy_ent], feed_dict={sy_ob_no:ob_no, old_mean_na:old_mean, old_std_a:old_std})
--
--
--        # KL
--        if kl > desired_kl * 2: 
--            stepsize /= 1.5 
--            print('stepsize -> %s'%stepsize)
--        elif kl < desired_kl / 2: 
--            stepsize *= 1.5
--            print('stepsize -> %s'%stepsize)
--        else:
--            print('stepsize OK')
--
--        # Log diagnostics
--        logz.log_tabular("EpRewMean", np.mean([path["reward"].sum() for path in paths]))
--        logz.log_tabular("EpLenMean", np.mean([pathlength(path) for path in paths]))
--        # logz.log_tabular("std", old_std)
--        logz.log_tabular("KLOldNew", kl)
--        logz.log_tabular("Entropy", ent)
--        logz.log_tabular("EVBefore", explained_variance_1d(vpred_n, vtarg_n))
--        logz.log_tabular("EVAfter", explained_variance_1d(vf.predict(ob_no), vtarg_n))
--        logz.log_tabular("TimestepsSoFar", total_timesteps)
--        # If you're overfitting, EVAfter will be way larger than EVBefore.
--        # Note that we fit value function AFTER using it to compute the advantage function to avoid introducing bias
--        logz.dump_tabular()
--
--
--def main_pendulum1(d):
--    return main_pendulum(**d)
--
--if __name__ == "__main__":
--
--    general_params = dict(gamma=0.97, animate=False, min_timesteps_per_batch=400, n_iter=3000, initial_stepsize=1e-7)
--    main_pendulum(logdir='./quad/', seed=2, desired_kl=2e-3, vf_type='linear', vf_params={}, **general_params)
--
--    # params = [
--    #     # dict(logdir='/tmp/ref/linearvf-kl2e-3-seed0', seed=0, desired_kl=2e-3, vf_type='linear', vf_params={}, **general_params),
--    #     # dict(logdir='/tmp/ref/nnvf-kl2e-3-seed0', seed=0, desired_kl=2e-3, vf_type='nn', vf_params=dict(n_epochs=10, stepsize=1e-3), **general_params),
--    #     # dict(logdir='/tmp/ref/linearvf-kl2e-3-seed1', seed=1, desired_kl=2e-3, vf_type='linear', vf_params={}, **general_params),
--    #     # dict(logdir='/tmp/ref/nnvf-kl2e-3-seed1', seed=1, desired_kl=2e-3, vf_type='nn', vf_params=dict(n_epochs=10, stepsize=1e-3), **general_params),
--    #     # dict(logdir='./experiment/linearvf-kl2e-3-seed2', seed=2, desired_kl=2e-3, vf_type='linear', vf_params={}, **general_params),
--    #     # dict(logdir='/tmp/ref/nnvf-kl2e-3-seed2', seed=2, desired_kl=2e-3, vf_type='nn', vf_params=dict(n_epochs=10, stepsize=1e-3), **general_params),
--    # ]
--    # import multiprocessing
--    # p = multiprocessing.Pool()
--    # p.map(main_pendulum1, params)
-\ No newline at end of file
-diff --git a/TRPO/quad/a.diff b/TRPO/quad/a.diff
-index 731cd8c..b3ac4c6 100644
---- a/TRPO/quad/a.diff
-+++ b/TRPO/quad/a.diff
-@@ -1,1306 +0,0 @@
--diff --git a/.DS_Store b/.DS_Store
--index 8305d60..1976ffd 100644
--Binary files a/.DS_Store and b/.DS_Store differ
--diff --git a/TRPO/main.py b/TRPO/main.py
--index 639a5d7..5f1bdc1 100644
----- a/TRPO/main.py
--+++ b/TRPO/main.py
--@@ -112,17 +112,18 @@ class LinearValueFunction(object):
-- def main_pendulum(logdir, seed, n_iter, gamma, min_timesteps_per_batch, initial_stepsize, desired_kl, vf_type, vf_params, animate=False):
--     tf.set_random_seed(seed)
--     np.random.seed(seed)
---    env  = QuadCopter(SIM_TIME_STEP, max_time=1, inverted_pendulum=False)
--+    env  = QuadCopter(SIM_TIME_STEP, inverted_pendulum=False)
--     ob_dim = env.stateSpace
--     ac_dim = env.actionSpace
--+    ac_lim = env.actionLimit
--     print("Quadcopter created")
--     print('state_dim: ', ob_dim)
--     print('action_dim: ', ac_dim)
---    # print('action_limit: ',action_limit)
--+    print('action_limit: ',ac_lim)
--     print('max time: ', MAX_EP_TIME)
--     print('max step: ',MAX_EP_STEPS)        
-- 
---    hover_position = np.asarray([0, 10, 0])
--+    hover_position = np.asarray([0, 0, 0])
--     task = hover(hover_position)
-- 
--     logz.configure_output_dir(logdir)
--@@ -143,14 +144,17 @@ def main_pendulum(logdir, seed, n_iter, gamma, min_timesteps_per_batch, initial_
-- 
--     # mean_na = dense(sy_h1, ac_dim, "mean", weight_init=normc_initializer(0.05)) # "logits", describing probability distribution of final layer
--     
---    mean_na = tf.tanh(dense(sy_h2, ac_dim, "final",weight_init=normc_initializer(0.1)))*5 # Mean control output
--+    mean_na = tf.tanh(dense(sy_h2, ac_dim, "final",weight_init=normc_initializer(0.1)))*ac_lim # Mean control output
--     # std_a = tf.constant(1.0, dtype=tf.float32, shape=[ac_dim])
---    std_a =  tf.get_variable("logstdev", [ac_dim], initializer=tf.ones_initializer())*0.1 
--+    std_a =  tf.get_variable("logstdev", [ac_dim], initializer=tf.ones_initializer())
-- 
--     # std_a = tf.constant(1.0,  shape=[ac_dim], dtype=tf.float32)
-- 
--     sy_sampled_ac = sample_gaussian(ac_dim, mean_na, std_a) # sampled actions, used for defining the policy (NOT computing the policy gradient)
---    sy_prob_n = (1.0/(std_a*2.5067)) * tf.exp(-0.5*tf.square((sy_ac_n - mean_na)/std_a))
--+    # sy_sampled_ac = tf.zeros([1, ac_dim])
--+    sy_prob_n = (1.0/tf.sqrt((tf.square(std_a)*2*3.1415926))) * tf.exp(-0.5*tf.square((sy_ac_n - mean_na)/std_a))
--+    # sy_prob_n = (1.0/(std_a*2.5067)) * tf.exp(-0.5*tf.square((sy_ac_n - mean_na)/std_a))
--+
--     sy_logprob_n = tf.log(sy_prob_n)
--     # sub = tf.subtract(sy_ac_n, mean_na)
--     # mul = tf.multiply(sub, sy_h1)
--@@ -179,9 +183,11 @@ def main_pendulum(logdir, seed, n_iter, gamma, min_timesteps_per_batch, initial_
--     total_timesteps = 0
--     stepsize = initial_stepsize
--     for i in range(n_iter):
--+
--         print("********** Iteration %i ************"%i)
-- 
--         # Collect paths until we have enough timesteps
--+
--         timesteps_this_batch = 0
--         paths = []
--         while True:
--@@ -193,6 +199,8 @@ def main_pendulum(logdir, seed, n_iter, gamma, min_timesteps_per_batch, initial_
--                 j += 1
--                 ob = ob.reshape(ob.shape[0],)
--                 obs.append(ob)
--+                # print ob
--+                # mean = sess.run(mean_na, feed_dict={sy_ob_no : ob[None]})[0]
--                 ac = sess.run(sy_sampled_ac, feed_dict={sy_ob_no : ob[None]})[0]
--                 # print ac
--                 ob, done, _ = env.step(ac)
--@@ -241,6 +249,8 @@ def main_pendulum(logdir, seed, n_iter, gamma, min_timesteps_per_batch, initial_
--         vf.fit(ob_no, vtarg_n)
-- 
--         # Policy update
--+        # print standardized_adv_n
--+        surr, adv, logp = sess.run([sy_surr, sy_adv_n, sy_prob_n], feed_dict={sy_ob_no:ob_no, sy_ac_n:ac_n, sy_adv_n:standardized_adv_n, sy_stepsize:stepsize})
--         _, old_mean, old_std = sess.run([update_op, mean_na, std_a], feed_dict={sy_ob_no:ob_no, sy_ac_n:ac_n, sy_adv_n:standardized_adv_n, sy_stepsize:stepsize})
--         kl, ent = sess.run([sy_kl, sy_ent], feed_dict={sy_ob_no:ob_no, old_mean_na:old_mean, old_std_a:old_std})
-- 
--@@ -274,7 +284,7 @@ def main_pendulum1(d):
-- 
-- if __name__ == "__main__":
-- 
---    general_params = dict(gamma=0.97, animate=False, min_timesteps_per_batch=2500, n_iter=300, initial_stepsize=1e-3)
--+    general_params = dict(gamma=0.97, animate=False, min_timesteps_per_batch=200, n_iter=3000, initial_stepsize=1e-4)
--     main_pendulum(logdir='./quad/', seed=2, desired_kl=2e-3, vf_type='linear', vf_params={}, **general_params)
-- 
--     # params = [
--diff --git a/TRPO/quad/a.diff b/TRPO/quad/a.diff
--index 0edef21..c994f12 100644
----- a/TRPO/quad/a.diff
--+++ b/TRPO/quad/a.diff
--@@ -1,1014 +0,0 @@
---diff --git a/.DS_Store b/.DS_Store
---index 181a197..8305d60 100644
---Binary files a/.DS_Store and b/.DS_Store differ
---diff --git a/run_simulator.py b/run_simulator.py
---index 99abc8d..cab5b2a 100644
------ a/run_simulator.py
---+++ b/run_simulator.py
---@@ -45,18 +45,18 @@ def plot_states(states):
---     plt.show()
--- 
--- def main():
----    quad  = QuadCopter()
----    time  = 1 # sec
---+    quad  = QuadCopter(inverted_pendulum=False)
---+    time  = 5 # sec
---     steps = int(time/quad.Ts)
----    delta  = [2, 2, 0.9, 0.9]
---+    delta  = [2, 2, 2, 2]
--- 
---     print "Simulate %i sec need total %i steps" %(time, steps)
--- 
---     states = np.zeros([steps, quad.stateSpace])
---     for i in range(steps):
---         state ,_ ,_ = quad.step(delta)
---+
---         states[i] = state
----    print state
---     plot_states(states)
--- 
--- 
---diff --git a/simulator.py b/simulator.py
---index 212f173..4c703aa 100644
------ a/simulator.py
---+++ b/simulator.py
---@@ -8,29 +8,43 @@ import numpy as np
--- from scipy.integrate import odeint
--- 
--- class QuadCopter(object):
----    def __init__(self, Ts=0.01, inverted_pendulum=True):
---+    def __init__(self, Ts=0.01, max_time = 10, inverted_pendulum=True):
---     # simulator  step time
---         self.Ts          = Ts
---+        self.max_time = max_time
---         self.stateSpace  = 16
---         self.actionSpace = 4
----        self.actionLimit  = 2 # maximum rotor speed degree/s TBD
---+        self.actionLimit  = 5 # maximum rotor speed degree/s TBD
---         self.inverted_pendulum = inverted_pendulum
--- 
---     # physical parameters of airframe
---+        # self.gravity = 9.81
---+        # self.l       = 0.175  # m, Distance between rotor and center
---+        # self.pen_l   = 0.20  # m, the length of stick
---+        # self.k1      = 1.0      # propellers constant
---+        # self.k2      = 2.0      # propellers constant 
---+        # self.mass    = 0.5  # mass
---+        # self.Jx      = 2.32e-3
---+        # self.Jy      = 2.32e-3
---+        # self.Jz      = 4.00e-3
---+
---         self.gravity = 9.81
----        self.l       = 45.0/1000  # m, Distance between rotor and center
----        self.pen_l   = 45.0/1000  # m, the length of stick
----        self.k1      = 100.0      # propellers constant
----        self.k2      = 100.0      # propellers constant 
----        self.mass    = 28.0/1000  # mass
----        self.Jx      = 16.60e-6
----        self.Jy      = 16.60e-6
----        self.Jz      = 29.26e-6
---+        self.l = 0.2; # m, Distance between rotor and center
---+        self.pen_l   = 0.20  # m, the length of stick
---+        self.k1 = 100; # propellers constant
---+        self.k2 = 100; # propellers constant 
---+        self.R = 0.04; # m, Center mass radius 
---+        self.M = 1 # kg, Body weight
---+        self.m = 0.07 #kg, Rotor weight 
---+        self.mass = self.M + self.m;
---+        self.Jx   = 2*self.M*self.R**2/5 + 2*self.l*self.m;
---+        self.Jy   = 2*self.M*self.R**2/5 + 2*self.l*self.m;
---+        self.Jz   = 2*self.M*self.R**2/5 + 4*self.l*self.m;
--- 
---     # initial conditions
---         self.pn0    = 0.0  # initial North position
---         self.pe0    = 0.0  # initial East position
----        self.pd0    = -10.0  # initial Down position (negative altitude)
---+        self.pd0    = 0.0  # initial Down position (negative altitude)
---         self.u0     = 0.0  # initial velocity along body x-axis
---         self.v0     = 0.0  # initial velocity along body y-axis
---         self.w0     = 0.0  # initial velocity along body z-axis
---@@ -47,6 +61,17 @@ class QuadCopter(object):
---         self.pen_vx0   = 0.0 # initial velocity along iv in vehicle frame
---         self.pen_vy0   = 0.0 # initial velocity along jv in vehicle frame
--- 
---+    # maximum conditions
---+        self.pn_max    =  100  # max North position
---+        self.pe_max    =  100  # max East position
---+        self.pd_max    =  100  # max Down position (negative altitude)
---+        self.u_max     = 10 # max velocity along body x-axis
---+        self.v_max     = 10 # max velocity along body y-axis
---+        self.w_max     = 10 # max velocity along body z-axis
---+        self.p_max     = 10 # max body frame roll rate
---+        self.q_max     = 10 # max body frame pitch rate
---+        self.r_max     = 10 # max body frame yaw rate
---+
---     # apply initial conditions
---         self.reset()
--- 
---@@ -75,11 +100,11 @@ class QuadCopter(object):
---         return self.states
--- 
---     def force(self, x):
----        f = 2.130295e-11*x**2.0 + 1.032633e-6*x + 5.484560e-4
---+        f = self.k1 * x
---         return f 
--- 
---     def torque(self, x):
----        tau = 0.005964552*self.force(x) + 1.563383e-5
---+        tau = self.k2 * x
---         return tau
--- 
---     def trunc_error(self,x):
---@@ -100,10 +125,11 @@ class QuadCopter(object):
--- 
---         uu = np.asarray([self.trunc_error(Force_x), self.trunc_error(Force_y), self.trunc_error(Force_z),
---                          self.trunc_error(Torque_x), self.trunc_error(Torque_y), self.trunc_error(Torque_z)])
----        # print uu
---+
---         return uu
--- 
--- 
---+
---     def Derivative(self, states, t, delta_f, delta_r, delta_b, delta_l):
---     # state variables
---         pn     = states[0]    
---@@ -213,15 +239,15 @@ class QuadCopter(object):
---         terminated = False
---         info = 'normal'
--- 
----        delta   = np.asarray(delta) * 37286.9359183576
---+        delta   = np.asarray(delta)*3.1416/180
---         delta_f = delta[0]
---         delta_r = delta[1]
---         delta_b = delta[2]
---         delta_l = delta[3]
--- 
---     # integral, ode
----        sol = odeint(self.Derivative, self.states, [self.time, self.time+self.Ts], args=(delta_f,delta_r,delta_b,delta_l), full_output=False, printmessg=False)
----        # sol = self.naive_int(self.Derivative, self.states, self.Ts, [delta_f,delta_r,delta_b,delta_l])
---+        # sol = odeint(self.Derivative, self.states, [self.time, self.time+self.Ts], args=(delta_f,delta_r,delta_b,delta_l), full_output=False, printmessg=False)
---+        sol = self.naive_int(self.Derivative, self.states, self.Ts, [delta_f,delta_r,delta_b,delta_l])
--- 
---         self.pn     = sol[1,0] 
---         self.pe     = sol[1,1] 
---@@ -241,11 +267,59 @@ class QuadCopter(object):
---         self.pen_vy = sol[1,15]
---         self.time   += self.Ts
--- 
----    # Fail condition check
----        if self.pd0 > 0:
---+        # check boundry condition
---+        if self.pn > self.pn_max:
---+            self.pn = self.pn_max
---+        if self.pn < -self.pn_max:
---+            self.pn = -self.pn_max
---+
---+        if self.pe > self.pe_max:
---+            self.pe = self.pe_max
---+        if self.pe < -self.pe_max:
---+            self.pe = -self.pe_max
---+
---+        if self.pd > self.pd_max:
---+            self.pd = self.pd_max
---+        if self.pd < -self.pd_max:
---+            self.pd = -self.pd_max
---+
---+        if self.u > self.u_max:
---+            self.u = self.u_max
---+        if self.u < -self.u_max:
---+            self.u = -self.u_max
---+
---+        if self.v > self.v_max:
---+            self.v = self.v_max
---+        if self.v < -self.v_max:
---+            self.v = -self.v_max
---+
---+        if self.w > self.w_max:
---+            self.w = self.w_max
---+        if self.w < -self.w_max:
---+            self.w = -self.w_max
---+
---+        if self.p > self.p_max:
---+            self.p = self.p_max
---+        if self.p < -self.p_max:
---+            self.p = -self.p_max
---+
---+        if self.q > self.q_max:
---+            self.q = self.q_max
---+        if self.q < -self.q_max:
---+            self.q = -self.q_max
---+
---+        if self.r > self.r_max:
---+            self.r = self.r_max
---+        if self.r < -self.r_max:
---+            self.r = -self.r_max
---+
---+        if self.time > self.max_time:
---             terminated = True
----            info = 'crash'   
----            print info         
---+            info = 'timeout'   
---+    # # Fail condition check
---+    #     if self.pd > 0:
---+    #         terminated = True
---+    #         info = 'crash'   
--- 
---         # print 'Time = %f' %self.time
---         self.states = np.asarray([self.pn, self.pe, self.pd, self.u, self.v, self.w, self.phi, self.theta, self.psi,
---diff --git a/simulator.pyc b/simulator.pyc
---index c4e33b1..abe164a 100644
---Binary files a/simulator.pyc and b/simulator.pyc differ
---diff --git a/train_quadcopter.py b/train_quadcopter.py
---index f329d6f..9b69b6f 100644
------ a/train_quadcopter.py
---+++ b/train_quadcopter.py
---@@ -1,16 +1,14 @@
--- # This file is aim to train the quadcopter keet at a constant position
--- 
--- import numpy as np
----import tflearn
--- import time
--- from simulator import QuadCopter
--- from replay_buffer import ReplayBuffer
--- from  util import *
---+from quad_task import *
--- import tensorflow as tf
--- import tensorflow.contrib.layers as layers
----import sys, os
----
----
---+import tflearn
--- 
--- # ==========================
--- #   Training Parameters
---@@ -20,17 +18,17 @@ SIM_TIME_STEP = 0.01
--- # Max training steps
--- MAX_EPISODES = 50000
--- # Max episode length
----MAX_EP_TIME = 2 # second
---+MAX_EP_TIME = 20 # second
--- MAX_EP_STEPS = int(MAX_EP_TIME/SIM_TIME_STEP)
--- # Explore decay rate
--- EXPLORE_INIT = 1
--- EXPLORE_DECAY = 0.99
----EXPLORE_MIN = 0.1
---+EXPLORE_MIN = 0.005
--- 
--- # Base learning rate for the Actor network
----ACTOR_LEARNING_RATE = 1e-4
---+ACTOR_LEARNING_RATE = 1e-6
--- # Base learning rate for the Critic Network
----CRITIC_LEARNING_RATE = 1e-3
---+CRITIC_LEARNING_RATE = 1e-5
--- # Discount factor 
--- GAMMA = 0.99
--- # Soft target update param
---@@ -42,7 +40,7 @@ TAU = 0.001
--- 
--- # Directory for storing tensorboard summary results
--- SUMMARY_DIR = './results/ddpg/'
----SAVE_STEP = 10
---+SAVE_STEP = 50
--- 
--- RANDOM_SEED = 1234
--- # Size of replay buffer
---@@ -95,29 +93,29 @@ class ActorNetwork(object):
---         self.num_trainable_vars = len(self.network_params) + len(self.target_network_params)
--- 
---     def create_actor_network(self): 
----        inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim], name='state')
----        net = tf.layers.batch_normalization(inputs)
----        net = layers.fully_connected(inputs, num_outputs=400, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
----        net = tf.layers.batch_normalization(net)
----
----        net = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
----        net = tf.layers.batch_normalization(net)
----        # net = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.th)
----        out_w = tf.Variable(np.random.randn(300, self.a_dim)*3e-3, dtype=tf.float32, name="out_w")
----        out_b = tf.Variable(tf.zeros([self.a_dim]), dtype=tf.float32, name="out_b")
----        out = tf.tanh(tf.matmul(net, out_w) +out_b)
----        scaled_out = tf.multiply(out, self.action_limit)# Scale output to -action_limit to action_limit
----
----        ## tflearn version
----        # inputs = tflearn.input_data(shape=[None, self.s_dim])
----        # net = tflearn.fully_connected(inputs, 400, activation='relu')
----        # net = tflearn.fully_connected(net, 300, activation='relu')
----        # # Final layer weights are init to Uniform[-3e-3, 3e-3]
----        # w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
----        # out = tflearn.fully_connected(
----        #     net, self.a_dim, activation='tanh', weights_init=w_init)
----        # # Scale output to -action_bound to action_bound
----        # scaled_out = tf.multiply(out, self.action_limit)
---+        # inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim], name='state')
---+        # net = tf.layers.batch_normalization(inputs)
---+        # net = layers.fully_connected(inputs, num_outputs=400 ,activation_fn=tf.nn.relu)
---+        # net = tf.layers.batch_normalization(net)
---+
---+        # net = layers.fully_connected(net, num_outputs=300 ,activation_fn=tf.nn.relu)
---+        # net = tf.layers.batch_normalization(net)
---+        # # net = layers.fully_connected(net, num_outputs=300 ,activation_fn=tf.th)
---+        # out_w = tf.Variable(np.random.randn(300, self.a_dim)*3e-3, dtype=tf.float32, name="out_w")
---+        # out_b = tf.Variable(tf.zeros([self.a_dim]), dtype=tf.float32, name="out_b")
---+        # out = tf.tanh(tf.matmul(net, out_w) + out_b)
---+        # scaled_out = tf.multiply(out, self.action_limit)# Scale output to -action_limit to action_limit
---+
---+        # tflearn version
---+        inputs = tflearn.input_data(shape=[None, self.s_dim])
---+        net = tflearn.fully_connected(inputs, 400, activation='relu')
---+        net = tflearn.fully_connected(net, 300, activation='relu')
---+        # Final layer weights are init to Uniform[-3e-3, 3e-3]
---+        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
---+        out = tflearn.fully_connected(
---+            net, self.a_dim, activation='tanh', weights_init=w_init)
---+        # Scale output to -action_bound to action_bound
---+        scaled_out = tf.multiply(out, self.action_limit)
--- 
--- 
---         return inputs, out, scaled_out 
---@@ -185,41 +183,40 @@ class CriticNetwork(object):
---         self.action_grads = tf.gradients(self.out, self.action)
--- 
---     def create_critic_network(self):
----        inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim])
----        action = tf.placeholder(dtype=tf.float32, shape=[None, self.a_dim])
---+        # inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim])
---+        # action = tf.placeholder(dtype=tf.float32, shape=[None, self.a_dim])
--- 
----        net = layers.fully_connected(inputs, num_outputs=400, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
----        # Add the action tensor in the 2nd hidden layer
----        # Use two temp layers to get the corresponding weights and biases 
----        t1 = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer(), activation_fn=None)
----        t2 = layers.fully_connected(action, num_outputs=300, weights_initializer=layers.xavier_initializer(), activation_fn=None)
---+        # net = layers.fully_connected(inputs, num_outputs=400 ,activation_fn=tf.nn.relu)
---+        # # Add the action tensor in the 2nd hidden layer
---+        # # Use two temp layers to get the corresponding weights and biases 
---+        # t1 = layers.fully_connected(net, num_outputs=300, activation_fn=None)
---+        # t2 = layers.fully_connected(action, num_outputs=300, activation_fn=None)
--- 
----        net = tf.nn.relu(t1 + t2)
----        net = tf.layers.batch_normalization(net)
----        out_w = tf.Variable(np.random.randn(300, 1)*3e-3, dtype=tf.float32)
----        out_b = tf.Variable(tf.zeros([1]), dtype=tf.float32, name="out_b")
---+        # net = tf.nn.relu(t1 + t2)
---+        # net = tf.layers.batch_normalization(net)
---+        # out_w = tf.Variable(np.random.randn(300, 1)*3e-3, dtype=tf.float32)
---+        # out_b = tf.Variable(tf.zeros([1]), dtype=tf.float32, name="out_b")
--- 
----        out = tf.matmul(net, out_w) + out_b
---+        # out = tf.matmul(net, out_w) + out_b
--- 
----        # out = layers.fully_connected(net, num_outputs=1, weights_initializer=layers.xavier_initializer(), activation_fn=None)
--- 
---         # tflearn version
----        # inputs = tflearn.input_data(shape=[None, self.s_dim])
----        # action = tflearn.input_data(shape=[None, self.a_dim])
----        # net = tflearn.fully_connected(inputs, 400, activation='relu')
---+        inputs = tflearn.input_data(shape=[None, self.s_dim])
---+        action = tflearn.input_data(shape=[None, self.a_dim])
---+        net = tflearn.fully_connected(inputs, 400, activation='relu')
--- 
---         # Add the action tensor in the 2nd hidden layer
---         # Use two temp layers to get the corresponding weights and biases
----        # t1 = tflearn.fully_connected(net, 300)
----        # t2 = tflearn.fully_connected(action, 300)
---+        t1 = tflearn.fully_connected(net, 300)
---+        t2 = tflearn.fully_connected(action, 300)
--- 
----        # net = tflearn.activation(
----        #     tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')
---+        net = tflearn.activation(
---+            tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')
--- 
----        # # linear layer connected to 1 output representing Q(s,a)
----        # # Weights are init to Uniform[-3e-3, 3e-3]
----        # w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
----        # out = tflearn.fully_connected(net, 1, weights_init=w_init)
---+        # linear layer connected to 1 output representing Q(s,a)
---+        # Weights are init to Uniform[-3e-3, 3e-3]
---+        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
---+        out = tflearn.fully_connected(net, 1, weights_init=w_init)
---         return inputs, action, out
--- 
---     def train(self, inputs, action, predicted_q_value):
---@@ -255,7 +252,7 @@ class CriticNetwork(object):
--- # ===========================
--- def build_summaries(): 
---     success_rate = tf.Variable(0.)
----    tf.summary.scalar("Success Rate", success_rate)
---+    tf.summary.scalar("Reward ", success_rate)
---     episode_ave_max_q = tf.Variable(0.)
---     tf.summary.scalar("Qmax Value", episode_ave_max_q)
--- 
---@@ -275,35 +272,14 @@ def count_parameters():
---         total_parameters += variable_parametes
---     print("total_parameters:", total_parameters)
--- 
----# ===========================
----#   Reward functions
----# ===========================
----def reward_function_hover_decorator(hover_position_set):
----
----    def reward_function_hover(states, terminal, info):
----        # this function is aimed to let the quadcopter hover in a certain position.
----        # e.g
----        # hover_position = np.asarray([0, 0, 0]) # pn = 0, pe = 0, pd = 0
----
----        if terminal:
----            reward = -500
----            print "terminated " , info
----        else:
----            current_position = states[0:3]
----            # reward function = -MSE(current_position, hover_position)
----            reward = -np.mean((current_position - hover_position_set)**2) + 200
----            # print reward
----
----        return reward
--- 
----    return reward_function_hover
--- 
--- # ===========================
--- #   Agent Training
--- # ===========================
--- 
--- 
----def train(sess, env, actor, critic, reward_fc):
---+def train(sess, env, actor, critic, task):
--- 
---     # Set up summary Ops
---     summary_ops, summary_vars = build_summaries()
---@@ -350,22 +326,29 @@ def train(sess, env, actor, critic, reward_fc):
---         states = np.zeros([MAX_EP_STEPS, env.stateSpace])
--- 
---         if i % SAVE_STEP == 0: # save check point every xx episode
----            sess.run(global_step.assign(i))
---+            # sess.run(global_step.assign(i))
---             save_path = saver.save(sess, SUMMARY_DIR + "model.ckpt" , global_step = i)
---             print("Model saved in file: %s" % save_path)
--- 
---         for j in xrange(MAX_EP_STEPS):
--- 
---             # Added exploration noise
----            exp = np.random.rand() * explore * env.actionLimit
---+            # exp = np.random.rand(1, 4) * explore * env.actionLimit
---+            exp = np.random.rand(1, 4) * explore * env.actionLimit
---+
---             a = actor.predict(np.reshape(s, (1, 16))) + exp
---+            # a = [[2,2,2,2]]
---+     
---             # a = actor.predict(np.reshape(s, (1, 16))) + (1. / (1. + i))
----
---             s2, terminal, info = env.step(a[0])
---+            # print 's', s
---+            # print 's2', s2
---+            # print j
---+            # print "action: ", a[0]
---+            # print "state: ", s2
---             states[j] = s2
--- 
----            r = reward_fc(s2, terminal, info) # calculate reward basec on s2
----
---+            r = task.reward(s2, terminal, info) # calculate reward basec on s2
---             replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r, \
---                 terminal, np.reshape(s2, (actor.s_dim,)))
--- 
---@@ -399,40 +382,40 @@ def train(sess, env, actor, critic, reward_fc):
---                 actor.update_target_network()
---                 critic.update_target_network()
--- 
----            s = s2
---             ep_reward += r
--- 
----            if terminal or j == MAX_EP_STEPS-1 or r < -10000:
---+            if terminal :
---                 # if i > 30:
---                 #     plot_states(states)
--- 
---                 print s[0:3]
---                 time_gap = time.time() - tic
--- 
----                # summary_str = sess.run(summary_ops, feed_dict={
----                #     summary_vars[0]: ep_reward,
----                #     summary_vars[1]: ep_ave_max_q / float(j)
----                # })
---+                summary_str = sess.run(summary_ops, feed_dict={
---+                    summary_vars[0]: (ep_reward/(j+1)),
---+                    summary_vars[1]: (ep_ave_max_q / float(j+1)),
---+                })
--- 
----                # writer.add_summary(summary_str, i)
----                # writer.flush()
---+                writer.add_summary(summary_str, i)
---+                writer.flush()
---                 # if ep_reward < last_epreward and last_epreward != 0:
---                 #     actor.learning_rate /= 10
---                 #     critic.learning_rate /= 10
---                 #     print "lr decay to ", actor.learning_rate
---                 last_epreward = ep_reward
----                print '| Reward: %.2f' % int(ep_reward/(j+1)), " | Episode", i, \
---+                print '| Reward: %.2f' % (ep_reward/(j+1)), " | Episode", i, \
---                         '| Qmax: %.4f' % (ep_ave_max_q / float(j+1)), ' | Time: %.2f' %(time_gap)
---                 tic = time.time()
--- 
---                 break
---+            s = np.copy(s2)
--- 
--- 
--- def main(_):
---         
---         np.random.seed(RANDOM_SEED)
---         tf.set_random_seed(RANDOM_SEED)
----        env  = QuadCopter(SIM_TIME_STEP, inverted_pendulum=False)
---+        env  = QuadCopter(SIM_TIME_STEP, max_time = 2, inverted_pendulum=False)
--- 
---         state_dim = env.stateSpace
---         action_dim = env.actionSpace
---@@ -445,8 +428,8 @@ def main(_):
---         print('max time: ', MAX_EP_TIME)
---         print('max step: ',MAX_EP_STEPS)        
--- 
----        hover_position = np.asarray([0, 0, -50])
----        reward_fc = reward_function_hover_decorator(hover_position)
---+        hover_position = np.asarray([0, 0, -10])
---+        task = hover(hover_position)
---         config = tf.ConfigProto()
---         config.gpu_options.allow_growth = True
--- 
---@@ -457,7 +440,7 @@ def main(_):
---             critic = CriticNetwork(sess, state_dim, action_dim, \
---                 CRITIC_LEARNING_RATE, TAU, actor.get_num_trainable_vars())
--- 
----            train(sess, env, actor, critic, reward_fc)
---+            train(sess, env, actor, critic, task)
--- 
--- if __name__ == '__main__':
---     tf.app.run()
---diff --git a/train_quadcopter_horizontalmove.py b/train_quadcopter_horizontalmove.py
---deleted file mode 100644
---index f37f481..0000000
------ a/train_quadcopter_horizontalmove.py
---+++ /dev/null
---@@ -1,464 +0,0 @@
----# This file is aim to train the quadcopter keet at a constant position
----
----import numpy as np
----import tflearn
----import time
----from simulator import QuadCopter
----from replay_buffer import ReplayBuffer
----from  util import *
----import tensorflow as tf
----import tensorflow.contrib.layers as layers
----import sys, os
----
----
----
----# ==========================
----#   Training Parameters
----# =========================
----# Simulation step
----SIM_TIME_STEP = 0.01
----# Max training steps
----MAX_EPISODES = 50000
----# Max episode length
----MAX_EP_TIME = 3 # second
----MAX_EP_STEPS = int(MAX_EP_TIME/SIM_TIME_STEP)
----# Explore decay rate
----EXPLORE_INIT = 1
----EXPLORE_DECAY = 0.99
----EXPLORE_MIN = 0.1
----
----# Base learning rate for the Actor network
----ACTOR_LEARNING_RATE = 1e-4
----# Base learning rate for the Critic Network
----CRITIC_LEARNING_RATE = 1e-3
----# Discount factor 
----GAMMA = 0.99
----# Soft target update param
----TAU = 0.001
----
----# ===========================
----#   Utility Parameters
----# ===========================
----
----# Directory for storing tensorboard summary results
----SUMMARY_DIR = './results/horizontal/'
----SAVE_STEP = 10
----
----RANDOM_SEED = 1234
----# Size of replay buffer
----BUFFER_SIZE = 1e6
----MINIBATCH_SIZE = 64
----
----# ===========================
----#   Actor and Critic DNNs
----# ===========================
----class ActorNetwork(object):
----    """ 
----    Input to the network is the state, output is the action
----    under a deterministic policy.
----
----    """
----    def __init__(self, sess, state_dim, action_dim, action_limit, learning_rate, tau):
----        self.sess = sess
----        self.s_dim = state_dim
----        self.a_dim = action_dim
----        self.action_limit = action_limit
----        self.learning_rate = learning_rate
----        self.tau = tau
----
----        # Actor Network
----        self.inputs, self.out, self.scaled_out = self.create_actor_network()
----
----        self.network_params = tf.trainable_variables()
----
----        # Target Network
----        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()
----        
----        self.target_network_params = tf.trainable_variables()[len(self.network_params):]
----
----        # Op for periodically updating target network with online network weights
----        self.update_target_network_params = \
----            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) + \
----                tf.multiply(self.target_network_params[i], 1. - self.tau))
----                for i in range(len(self.target_network_params))]
----
----        # This gradient will be provided by the critic network
----        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])
----        
----        # Combine the gradients here 
----        self.actor_gradients = tf.gradients(self.scaled_out, self.network_params, -self.action_gradient)
----
----        # Optimization Op
----        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\
----            apply_gradients(zip(self.actor_gradients, self.network_params))
----
----        self.num_trainable_vars = len(self.network_params) + len(self.target_network_params)
----
----    def create_actor_network(self): 
----        inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim], name='state')
----        net = tf.layers.batch_normalization(inputs)
----        net = layers.fully_connected(inputs, num_outputs=400, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
----        net = tf.layers.batch_normalization(net)
----
----        net = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
----        net = tf.layers.batch_normalization(net)
----        # net = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.th)
----        out_w = tf.Variable(np.random.randn(300, self.a_dim)*3e-3, dtype=tf.float32, name="out_w")
----        out_b = tf.Variable(tf.zeros([self.a_dim]), dtype=tf.float32, name="out_b")
----        out = tf.tanh(tf.matmul(net, out_w) +out_b)
----        scaled_out = tf.multiply(out, self.action_limit)# Scale output to -action_limit to action_limit
----
----        ## tflearn version
----        # inputs = tflearn.input_data(shape=[None, self.s_dim])
----        # net = tflearn.fully_connected(inputs, 400, activation='relu')
----        # net = tflearn.fully_connected(net, 300, activation='relu')
----        # # Final layer weights are init to Uniform[-3e-3, 3e-3]
----        # w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
----        # out = tflearn.fully_connected(
----        #     net, self.a_dim, activation='tanh', weights_init=w_init)
----        # # Scale output to -action_bound to action_bound
----        # scaled_out = tf.multiply(out, self.action_limit)
----
----
----        return inputs, out, scaled_out 
----
----    def train(self, inputs, a_gradient):
----        self.sess.run(self.optimize, feed_dict={
----            self.inputs: inputs,
----            self.action_gradient: a_gradient
----        })
----
----    def predict(self, inputs):
----        return self.sess.run(self.scaled_out, feed_dict={
----            self.inputs: inputs
----        })
----
----    def predict_target(self, inputs):
----        return self.sess.run(self.target_scaled_out, feed_dict={
----            self.target_inputs: inputs
----        })
----
----    def update_target_network(self):
----        self.sess.run(self.update_target_network_params)
----
----    def get_num_trainable_vars(self):
----        return self.num_trainable_vars
----
----class CriticNetwork(object):
----    """ 
----    Input to the network is the state and action, output is Q(s,a).
----    The action must be obtained from the output of the Actor network.
----
----    """
----    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, num_actor_vars):
----        self.sess = sess
----        self.s_dim = state_dim
----        self.a_dim = action_dim
----        self.learning_rate = learning_rate
----        self.tau = tau
----
----        # Create the critic network
----        self.inputs, self.action, self.out = self.create_critic_network()
----
----        self.network_params = tf.trainable_variables()[num_actor_vars:]
----
----        # Target Network
----        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()
----        
----        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]
----
----        # Op for periodically updating target network with online network weights with regularization
----        self.update_target_network_params = \
----            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) + tf.multiply(self.target_network_params[i], 1. - self.tau))
----                for i in range(len(self.target_network_params))]
----    
----        # Network target (y_i)
----        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])
----
----        # Define loss and optimization Op
----        self.loss = tf.losses.mean_squared_error(self.predicted_q_value, self.out)
----        # self.loss = tflearn.mean_square(self.predicted_q_value, self.out)
----
----        self.optimize = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)
----
----        # Get the gradient of the net w.r.t. the action
----        self.action_grads = tf.gradients(self.out, self.action)
----
----    def create_critic_network(self):
----        inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim])
----        action = tf.placeholder(dtype=tf.float32, shape=[None, self.a_dim])
----
----        net = layers.fully_connected(inputs, num_outputs=400, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
----        # Add the action tensor in the 2nd hidden layer
----        # Use two temp layers to get the corresponding weights and biases 
----        t1 = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer(), activation_fn=None)
----        t2 = layers.fully_connected(action, num_outputs=300, weights_initializer=layers.xavier_initializer(), activation_fn=None)
----
----        net = tf.nn.relu(t1 + t2)
----        net = tf.layers.batch_normalization(net)
----        out_w = tf.Variable(np.random.randn(300, 1)*3e-3, dtype=tf.float32)
----        out_b = tf.Variable(tf.zeros([1]), dtype=tf.float32, name="out_b")
----
----        net = tf.matmul(net, out_w) + out_b
----        net = tf.layers.batch_normalization(net)
----
----        out = layers.fully_connected(net, num_outputs=1, weights_initializer=layers.xavier_initializer(), activation_fn=None)
----
----        # tflearn version
----        # inputs = tflearn.input_data(shape=[None, self.s_dim])
----        # action = tflearn.input_data(shape=[None, self.a_dim])
----        # net = tflearn.fully_connected(inputs, 400, activation='relu')
----
----        # Add the action tensor in the 2nd hidden layer
----        # Use two temp layers to get the corresponding weights and biases
----        # t1 = tflearn.fully_connected(net, 300)
----        # t2 = tflearn.fully_connected(action, 300)
----
----        # net = tflearn.activation(
----        #     tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')
----
----        # # linear layer connected to 1 output representing Q(s,a)
----        # # Weights are init to Uniform[-3e-3, 3e-3]
----        # w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
----        # out = tflearn.fully_connected(net, 1, weights_init=w_init)
----        return inputs, action, out
----
----    def train(self, inputs, action, predicted_q_value):
----        return self.sess.run([self.out, self.optimize], feed_dict={
----            self.inputs: inputs,
----            self.action: action,
----            self.predicted_q_value: predicted_q_value
----        })
----
----    def predict(self, inputs, action):
----        return self.sess.run(self.out, feed_dict={
----            self.inputs: inputs,
----            self.action: action
----        })
----
----    def predict_target(self, inputs, action):
----        return self.sess.run(self.target_out, feed_dict={
----            self.target_inputs: inputs,
----            self.target_action: action
----        })
----
----    def action_gradients(self, inputs, actions): 
----        return self.sess.run(self.action_grads, feed_dict={
----            self.inputs: inputs,
----            self.action: actions
----        })
----
----    def update_target_network(self):
----        self.sess.run(self.update_target_network_params)
----
----# ===========================
----#   Tensorflow Summary Ops
----# ===========================
----def build_summaries(): 
----    success_rate = tf.Variable(0.)
----    tf.summary.scalar("Success Rate", success_rate)
----    episode_ave_max_q = tf.Variable(0.)
----    tf.summary.scalar("Qmax Value", episode_ave_max_q)
----
----    summary_vars = [success_rate, episode_ave_max_q]
----    summary_ops = tf.summary.merge_all()
----
----    return summary_ops, summary_vars
----
----def count_parameters():
----    total_parameters = 0
----    for variable in tf.trainable_variables():
----        # shape is an array of tf.Dimension
----        shape = variable.get_shape()
----        variable_parametes = 1
----        for dim in shape:
----            variable_parametes *= dim.value
----        total_parameters += variable_parametes
----    print("total_parameters:", total_parameters)
----
----# ===========================
----#   Reward functions
----# ===========================
----def reward_function_hover_decorator(hover_position_set):
----
----    def reward_function_hover(states, terminal, info):
----        # this function is aimed to let the quadcopter hover in a certain position.
----        # e.g
----        # hover_position = np.asarray([0, 0, 0]) # pn = 0, pe = 0, pd = 0
----
----        if terminal:
----            reward = -500
----            print "terminated " , info
----        else:
----            current_position = states[0:3]
----            # reward function = -MSE(current_position, hover_position)
----            reward = -np.mean((current_position - hover_position_set)**2) + 200
----            # print reward
----
----        return reward
----
----    return reward_function_hover
----
----# ===========================
----#   Agent Training
----# ===========================
----
----
----def train(sess, env, actor, critic, reward_fc):
----
----    # Set up summary Ops
----    summary_ops, summary_vars = build_summaries()
----    global_step = tf.Variable(0, dtype=tf.int32)
----
----    sess.run(tf.global_variables_initializer())
----    writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)
----
----    # load model if have
----    saver = tf.train.Saver()
----    checkpoint = tf.train.get_checkpoint_state(SUMMARY_DIR)
----    
----    if checkpoint and checkpoint.model_checkpoint_path:
----        saver.restore(sess, checkpoint.model_checkpoint_path)
----        print ("Successfully loaded:", checkpoint.model_checkpoint_path)
----        print("global step: ", global_step.eval())
----
----    else:
----        print ("Could not find old network weights")
----
----    # Initialize target network weights
----    actor.update_target_network()
----    critic.update_target_network()
----    count_parameters()
----
----    # Initialize replay memory
----    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)
----    tic = time.time()
----    last_epreward = 0 
----    i = global_step.eval()
----
----    while True:
----        i += 1
----        if i > MAX_EPISODES:
----            break
----        print ("Iteration: ", i)
----        explore = EXPLORE_INIT*EXPLORE_DECAY**i
----        explore = max(EXPLORE_MIN, explore)
----        print ("explore: ", explore)
----        s = env.reset()
----
----        ep_reward = 0
----        ep_ave_max_q = 0
----        states = np.zeros([MAX_EP_STEPS, env.stateSpace])
----
----        if i % SAVE_STEP == 0: # save check point every xx episode
----            sess.run(global_step.assign(i))
----            save_path = saver.save(sess, SUMMARY_DIR + "model.ckpt" , global_step = i)
----            print("Model saved in file: %s" % save_path)
----
----        for j in xrange(MAX_EP_STEPS):
----
----            # Added exploration noise
----            exp = np.random.rand() * explore * env.actionLimit
----            a = actor.predict(np.reshape(s, (1, 16))) + exp
----            # a = actor.predict(np.reshape(s, (1, 16))) + (1. / (1. + i))
----
----            s2, terminal, info = env.step(a[0])
----            states[j] = s2
----
----            r = reward_fc(s2, terminal, info) # calculate reward basec on s2
----
----            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r, \
----                terminal, np.reshape(s2, (actor.s_dim,)))
----
----            # Keep adding experience to the memory until
----            # there are at least minibatch size samples
----            if replay_buffer.size() > MINIBATCH_SIZE:     
----                s_batch, a_batch, r_batch, t_batch, s2_batch = \
----                    replay_buffer.sample_batch(MINIBATCH_SIZE)
----
----                # Calculate targets
----                target_q = critic.predict_target(s2_batch, actor.predict_target(s2_batch))
----
----                y_i = []
----                for k in xrange(MINIBATCH_SIZE):
----                    if t_batch[k]:
----                        y_i.append(r_batch[k])
----                    else:
----                        y_i.append(r_batch[k] + GAMMA * target_q[k])
----
----                # Update the critic given the targets
----                predicted_q_value, _ = critic.train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)))
----            
----                ep_ave_max_q += np.amax(predicted_q_value)
----
----                # Update the actor policy using the sampled gradient
----                a_outs = actor.predict(s_batch)                
----                grads = critic.action_gradients(s_batch, a_outs)
----                actor.train(s_batch, grads[0])
----
----                # Update target networks
----                actor.update_target_network()
----                critic.update_target_network()
----
----            s = s2
----            ep_reward += r
----
----            if terminal or j == MAX_EP_STEPS-1 or r < -10000:
----                # if i > 30:
----                #     plot_states(states)
----
----                print s[0:3]
----                time_gap = time.time() - tic
----
----                # summary_str = sess.run(summary_ops, feed_dict={
----                #     summary_vars[0]: ep_reward,
----                #     summary_vars[1]: ep_ave_max_q / float(j)
----                # })
----
----                # writer.add_summary(summary_str, i)
----                # writer.flush()
----                # if ep_reward < last_epreward and last_epreward != 0:
----                #     actor.learning_rate /= 10
----                #     critic.learning_rate /= 10
----                #     print "lr decay to ", actor.learning_rate
----                last_epreward = ep_reward
----                print '| Reward: %.2f' % int(ep_reward/(j+1)), " | Episode", i, \
----                        '| Qmax: %.4f' % (ep_ave_max_q / float(j+1)), ' | Time: %.2f' %(time_gap)
----                tic = time.time()
----
----                break
----
----
----def main(_):
----        
----        np.random.seed(RANDOM_SEED)
----        tf.set_random_seed(RANDOM_SEED)
----        env  = QuadCopter(SIM_TIME_STEP, inverted_pendulum=False)
----
----        state_dim = env.stateSpace
----        action_dim = env.actionSpace
----        action_limit = env.actionLimit
----
----        print("Quadcopter created")
----        print('state_dim: ', state_dim)
----        print('action_dim: ', action_dim)
----        print('action_limit: ',action_limit)
----        print('max time: ', MAX_EP_TIME)
----        print('max step: ',MAX_EP_STEPS)        
----
----        hover_position = np.asarray([10, 10, 0])
----        reward_fc = reward_function_hover_decorator(hover_position)
----        config = tf.ConfigProto()
----        config.gpu_options.allow_growth = True
----
----        with tf.Session(config=config) as sess:
----            actor = ActorNetwork(sess, state_dim, action_dim, action_limit, \
----                ACTOR_LEARNING_RATE, TAU)
----
----            critic = CriticNetwork(sess, state_dim, action_dim, \
----                CRITIC_LEARNING_RATE, TAU, actor.get_num_trainable_vars())
----
----            train(sess, env, actor, critic, reward_fc)
----
----if __name__ == '__main__':
----    tf.app.run()
--diff --git a/TRPO/quad/log.txt b/TRPO/quad/log.txt
--index 4d414ea..e69de29 100644
----- a/TRPO/quad/log.txt
--+++ b/TRPO/quad/log.txt
--@@ -1,24 +0,0 @@
---EpRewMean	EpLenMean	KLOldNew	Entropy	EVBefore	EVAfter	TimestepsSoFar
---16518.9977502	100.0	111.035	-31.2201	0.0	0.999928494282	2600
---16560.0986612	100.0	30.8037	-26.6944	-30.0493322798	0.999810184661	5200
---16464.5782827	100.0	2.46095	-174.28	0.974063645406	0.999923340544	7800
---16469.4972005	100.0	2.25312	-167.653	0.9968422001	0.999893362521	10400
---16401.9172533	100.0	nan	nan	-0.0298791963379	0.999806785905	13000
---nan	100.0	nan	nan	nan	nan	15600
---nan	100.0	nan	nan	nan	nan	18200
---nan	100.0	nan	nan	nan	nan	20800
---nan	100.0	nan	nan	nan	nan	23400
---nan	100.0	nan	nan	nan	nan	26000
---nan	100.0	nan	nan	nan	nan	28600
---nan	100.0	nan	nan	nan	nan	31200
---nan	100.0	nan	nan	nan	nan	33800
---nan	100.0	nan	nan	nan	nan	36400
---nan	100.0	nan	nan	nan	nan	39000
---nan	100.0	nan	nan	nan	nan	41600
---nan	100.0	nan	nan	nan	nan	44200
---nan	100.0	nan	nan	nan	nan	46800
---nan	100.0	nan	nan	nan	nan	49400
---nan	100.0	nan	nan	nan	nan	52000
---nan	100.0	nan	nan	nan	nan	54600
---nan	100.0	nan	nan	nan	nan	57200
---nan	100.0	nan	nan	nan	nan	59800
--diff --git a/TRPO/quad_task.py b/TRPO/quad_task.py
--index b7a80cf..e7e506e 100644
----- a/TRPO/quad_task.py
--+++ b/TRPO/quad_task.py
--@@ -18,6 +18,6 @@ class hover(object):
--         current_position = states[0:3]
--         # last_position = state_last[0:3]
-- 
---        reward = -np.mean((current_position - self.hover_position_set)**2) + 200
--+        reward = -np.mean((current_position - self.hover_position_set)**2)
-- 
--         return reward
--diff --git a/TRPO/quad_task.pyc b/TRPO/quad_task.pyc
--index be2bc93..a78692c 100644
--Binary files a/TRPO/quad_task.pyc and b/TRPO/quad_task.pyc differ
--diff --git a/TRPO/simulator.py b/TRPO/simulator.py
--index ac46eb3..303670a 100644
----- a/TRPO/simulator.py
--+++ b/TRPO/simulator.py
--@@ -8,13 +8,13 @@ import numpy as np
-- from scipy.integrate import odeint
-- 
-- class QuadCopter(object):
---    def __init__(self, Ts=0.01, max_time = 10, inverted_pendulum=True):
--+    def __init__(self, Ts=0.01, max_time = 2, inverted_pendulum=True):
--     # simulator  step time
--         self.Ts          = Ts
--         self.max_time = max_time
--         self.stateSpace  = 16
--         self.actionSpace = 4
---        self.actionLimit  = 5 # maximum rotor speed degree/s TBD
--+        self.actionLimit  = 5.0 # maximum rotor speed degree/s TBD
--         self.inverted_pendulum = inverted_pendulum
-- 
--     # physical parameters of airframe
--@@ -62,9 +62,9 @@ class QuadCopter(object):
--         self.pen_vy0   = 0.0 # initial velocity along jv in vehicle frame
-- 
--     # maximum conditions
---        self.pn_max    =  20  # max North position
---        self.pe_max    =  20  # max East position
---        self.pd_max    =  20  # max Down position (negative altitude)
--+        self.pn_max    =  100  # max North position
--+        self.pe_max    =  100  # max East position
--+        self.pd_max    =  100  # max Down position (negative altitude)
--         self.u_max     = 10 # max velocity along body x-axis
--         self.v_max     = 10 # max velocity along body y-axis
--         self.w_max     = 10 # max velocity along body z-axis
--diff --git a/TRPO/simulator.pyc b/TRPO/simulator.pyc
--index 9dc4859..9ecdd5c 100644
--Binary files a/TRPO/simulator.pyc and b/TRPO/simulator.pyc differ
--diff --git a/results/ddpg/events.out.tfevents.1496822173.Zhenghaos-MacBook-Pro.local b/results/ddpg/events.out.tfevents.1496822173.Zhenghaos-MacBook-Pro.local
--deleted file mode 100644
--index 30a1eaf..0000000
--Binary files a/results/ddpg/events.out.tfevents.1496822173.Zhenghaos-MacBook-Pro.local and /dev/null differ
--diff --git a/results/ddpg/events.out.tfevents.1496822205.Zhenghaos-MacBook-Pro.local b/results/ddpg/events.out.tfevents.1496822205.Zhenghaos-MacBook-Pro.local
--deleted file mode 100644
--index 0765863..0000000
--Binary files a/results/ddpg/events.out.tfevents.1496822205.Zhenghaos-MacBook-Pro.local and /dev/null differ
--diff --git a/results/ddpg/events.out.tfevents.1496823044.Zhenghaos-MacBook-Pro.local b/results/ddpg/events.out.tfevents.1496823044.Zhenghaos-MacBook-Pro.local
--deleted file mode 100644
--index 9dc1a96..0000000
--Binary files a/results/ddpg/events.out.tfevents.1496823044.Zhenghaos-MacBook-Pro.local and /dev/null differ
--diff --git a/results/ddpg/events.out.tfevents.1496823209.Zhenghaos-MacBook-Pro.local b/results/ddpg/events.out.tfevents.1496823209.Zhenghaos-MacBook-Pro.local
--deleted file mode 100644
--index b664392..0000000
--Binary files a/results/ddpg/events.out.tfevents.1496823209.Zhenghaos-MacBook-Pro.local and /dev/null differ
--diff --git a/results/ddpg/events.out.tfevents.1496823333.Zhenghaos-MacBook-Pro.local b/results/ddpg/events.out.tfevents.1496823333.Zhenghaos-MacBook-Pro.local
--deleted file mode 100644
--index 8c93105..0000000
--Binary files a/results/ddpg/events.out.tfevents.1496823333.Zhenghaos-MacBook-Pro.local and /dev/null differ
--diff --git a/results/ddpg/events.out.tfevents.1496823343.Zhenghaos-MacBook-Pro.local b/results/ddpg/events.out.tfevents.1496823343.Zhenghaos-MacBook-Pro.local
--deleted file mode 100644
--index 9765d68..0000000
--Binary files a/results/ddpg/events.out.tfevents.1496823343.Zhenghaos-MacBook-Pro.local and /dev/null differ
--diff --git a/results/ddpg/events.out.tfevents.1496823390.Zhenghaos-MacBook-Pro.local b/results/ddpg/events.out.tfevents.1496823390.Zhenghaos-MacBook-Pro.local
--deleted file mode 100644
--index 983a2d9..0000000
--Binary files a/results/ddpg/events.out.tfevents.1496823390.Zhenghaos-MacBook-Pro.local and /dev/null differ
--diff --git a/results/ddpg/events.out.tfevents.1496823451.Zhenghaos-MacBook-Pro.local b/results/ddpg/events.out.tfevents.1496823451.Zhenghaos-MacBook-Pro.local
--deleted file mode 100644
--index 14912aa..0000000
--Binary files a/results/ddpg/events.out.tfevents.1496823451.Zhenghaos-MacBook-Pro.local and /dev/null differ
--diff --git a/results/ddpg/events.out.tfevents.1496823556.Zhenghaos-MacBook-Pro.local b/results/ddpg/events.out.tfevents.1496823556.Zhenghaos-MacBook-Pro.local
--deleted file mode 100644
--index f83f518..0000000
--Binary files a/results/ddpg/events.out.tfevents.1496823556.Zhenghaos-MacBook-Pro.local and /dev/null differ
--diff --git a/run_simulator.py b/run_simulator.py
--index cab5b2a..f94e83a 100644
----- a/run_simulator.py
--+++ b/run_simulator.py
--@@ -46,15 +46,17 @@ def plot_states(states):
-- 
-- def main():
--     quad  = QuadCopter(inverted_pendulum=False)
---    time  = 5 # sec
--+    time  = 10 # sec
--     steps = int(time/quad.Ts)
---    delta  = [2, 2, 2, 2]
--+    delta  = [1.6, 1.5, 1.4, 1.5]
-- 
--     print "Simulate %i sec need total %i steps" %(time, steps)
-- 
--     states = np.zeros([steps, quad.stateSpace])
--     for i in range(steps):
--         state ,_ ,_ = quad.step(delta)
--+        if i > 2:
--+            delta  = [1.5, 1.5, 1.5, 1.5]
-- 
--         states[i] = state
--     plot_states(states)
--diff --git a/simulator.py b/simulator.py
--index 4c703aa..fcb89ad 100644
----- a/simulator.py
--+++ b/simulator.py
--@@ -14,7 +14,7 @@ class QuadCopter(object):
--         self.max_time = max_time
--         self.stateSpace  = 16
--         self.actionSpace = 4
---        self.actionLimit  = 5 # maximum rotor speed degree/s TBD
--+        self.actionLimit  = 5.0 # maximum rotor speed degree/s TBD
--         self.inverted_pendulum = inverted_pendulum
-- 
--     # physical parameters of airframe
--diff --git a/train_quadcopter.py b/train_quadcopter.py
--index 353ac9e..32428e1 100644
----- a/train_quadcopter.py
--+++ b/train_quadcopter.py
--@@ -26,9 +26,9 @@ EXPLORE_DECAY = 0.99
-- EXPLORE_MIN = 0.005
-- 
-- # Base learning rate for the Actor network
---ACTOR_LEARNING_RATE = 1e-6
--+ACTOR_LEARNING_RATE = 1e-7
-- # Base learning rate for the Critic Network
---CRITIC_LEARNING_RATE = 1e-5
--+CRITIC_LEARNING_RATE = 1e-6
-- # Discount factor 
-- GAMMA = 0.99
-- # Soft target update param
--@@ -329,7 +329,7 @@ def train(sess, env, actor, critic, task):
--             # sess.run(global_step.assign(i))
--             save_path = saver.save(sess, SUMMARY_DIR + "model.ckpt" , global_step = i)
--             print("Model saved in file: %s" % save_path)
---            j = 0
--+
--         for j in xrange(MAX_EP_STEPS+1):
-- 
--             # Added exploration noise
--@@ -384,8 +384,8 @@ def train(sess, env, actor, critic, task):
-- 
--             ep_reward += r
--             if terminal:
---                # if i > 50:
---                #     plot_states(states)
--+                if i > 30:
--+                    plot_states(states)
-- 
--                 print s[0:3]
--                 time_gap = time.time() - tic
--@@ -423,7 +423,7 @@ def main(_):
--         print('max time: ', MAX_EP_TIME)
--         print('max step: ',MAX_EP_STEPS)        
-- 
---        hover_position = np.asarray([0, 0, 0])
--+        hover_position = np.asarray([3, 0, 0])
--         task = hover(hover_position)
--         config = tf.ConfigProto()
--         config.gpu_options.allow_growth = True
-diff --git a/TRPO/quad/log.txt b/TRPO/quad/log.txt
-index 7066414..e69de29 100644
---- a/TRPO/quad/log.txt
-+++ b/TRPO/quad/log.txt
-@@ -1,59 +0,0 @@
--EpRewMean	EpLenMean	KLOldNew	Entropy	EVBefore	EVAfter	TimestepsSoFar
---1.35776751534	200.0	6.14539e-05	0.0881252	0.0	0.99561640273	400
---0.679355285616	200.0	0.000463427	0.0361687	-991.25250809	0.977763318672	800
---0.488832490774	200.0	0.000609653	0.296448	-14.8047671602	0.947857831443	1200
---1.01756270377	200.0	0.00015495	0.151271	-2.69247711192	0.985169298176	1600
---0.71400012564	200.0	0.00697885	0.364659	-5011.58366208	0.975124981803	2000
---1.67991522554	200.0	0.000594365	0.432362	0.323935867966	0.969371118612	2400
---2.10105510391	200.0	0.00195566	0.51278	-13.2841098829	0.994109739683	2800
---3.52194967811	200.0	0.000768367	0.573803	-12.1418603734	0.99487098152	3200
---2.0221817636	200.0	0.00133225	0.582498	0.0107076790308	0.990894443821	3600
---3.99943399361	200.0	0.00210865	0.876263	0.527578025574	0.989949518344	4000
---2.58789620689	200.0	0.00165497	0.890348	-45.8571699525	0.990922623491	4400
---7.6184998965	200.0	0.000839981	0.95599	-0.0868528159903	0.994113831313	4800
---8.70364155723	200.0	0.00118069	1.11614	0.354672595985	0.995391006471	5200
---9.75668973555	200.0	0.00462028	0.657867	-770.667837134	0.994224550135	5600
---10.2670261522	200.0	0.000439178	1.32352	0.233865232072	0.993739532447	6000
---17.8214837465	200.0	0.00210888	1.20018	-0.266513774956	0.99809950759	6400
---10.1270936181	200.0	0.000966105	1.09167	-232.517993254	0.996063763226	6800
---10.1068668692	200.0	0.00157119	0.920953	0.239689149368	0.9978112023	7200
---24.3278646977	200.0	0.00166047	0.826763	-0.129725055975	0.997171736855	7600
---13.804355128	200.0	0.00329817	0.917161	0.778108183175	0.998555225861	8000
---15.0467409128	200.0	0.00081348	0.955093	-0.250341763537	0.99839745637	8400
---10.6169859527	200.0	0.00434565	1.19851	-3.13914374814	0.998293454368	8800
---24.9322183466	200.0	0.000399536	1.02593	-0.0054090474664	0.996922099275	9200
---22.0759745785	200.0	0.00892077	1.64384	0.799852070251	0.996746520966	9600
---22.398026832	200.0	0.00231154	1.62131	-1.16563014768	0.996382786878	10000
---23.7789502934	200.0	0.00681274	1.50421	-12.7900067761	0.998206315734	10400
---31.000788758	200.0	0.000145666	1.34026	0.661512613926	0.99749852127	10800
---5.73719588988	200.0	0.00210563	0.940086	-2.3187552832	0.997577998257	11200
---26.9358414727	200.0	0.00299005	1.32886	0.902699651826	0.997820462799	11600
---13.5920797519	200.0	0.00376897	1.03962	-3.44974961428	0.996567783524	12000
---19.5830492615	200.0	0.00358955	1.20972	0.209721386545	0.995381645477	12400
---16.4557220415	200.0	0.000591478	1.28735	-21.3590060041	0.99859929535	12800
---21.7301462144	200.0	0.00771449	0.968325	-312.226500278	0.997948530652	13200
---7.3187021339	200.0	0.00258162	0.610278	-2.42818634552	0.996576721222	13600
---11.1816069536	200.0	0.00120986	0.63259	-0.109868551176	0.997418618458	14000
---10.7649738522	200.0	0.0116275	0.531649	-0.665635420414	0.997606172751	14400
---18.9552507146	200.0	0.00624986	0.055649	0.582234343419	0.998231270418	14800
---14.4371551612	200.0	0.00290145	-0.484198	-33.1597344559	0.995551191747	15200
---7.3086849345	200.0	0.00872566	-1.18241	-3.23510274879	0.985417983437	15600
---14.4068838469	200.0	0.000832068	-1.15595	-8.65596867711	0.989395621512	16000
---13.9752442056	200.0	0.00141103	-1.06504	0.725228241841	0.99610891663	16400
---5.03952328157	200.0	0.000970674	-1.29597	-11.1884553908	0.994378492179	16800
---6.42557771813	200.0	0.00624642	-1.74933	0.100593005906	0.990700380782	17200
---5.38115261839	200.0	0.00245388	-1.69019	-1.55412392492	0.994451988355	17600
---9.9787083011	200.0	0.00144866	-2.02502	-0.051682537111	0.994266800069	18000
---10.9587589538	200.0	0.000544029	-1.61519	0.855847361319	0.996127467363	18400
---9.14336097016	200.0	0.00024264	-2.40668	-1.19099057999	0.998792345076	18800
---10.1811247764	200.0	0.00135432	-1.47706	-1.1247843205	0.995537236499	19200
---12.65670006	200.0	0.000238536	-1.97504	0.530842104279	0.995657182884	19600
---6.82214117829	200.0	0.000207064	-1.92854	-24.5076479545	0.997190486125	20000
---14.746029484	200.0	0.000798257	-2.39106	-0.0503459522082	0.99822085091	20400
---12.0249534112	200.0	0.00286194	-2.22503	0.3073837557	0.99684874448	20800
---7.81287530152	200.0	0.00889258	-1.66742	-10.0257793199	0.994654874235	21200
---14.4502347111	200.0	0.00470683	-1.49154	-0.98654477327	0.989719387123	21600
---15.7150462646	200.0	0.000511681	-2.02953	-2.78659881234	0.994524840322	22000
---14.1165289323	200.0	0.000356542	-2.42603	-135.999953612	0.997509531831	22400
---31.0115155201	200.0	0.00422575	-2.0383	-0.43837918594	0.996088613438	22800
---22.3737969068	200.0	0.00653033	-2.03554	-3.53856598435	0.998049107962	23200
-diff --git a/TRPO/quad_task.py b/TRPO/quad_task.py
-index 6465701..50de380 100644
---- a/TRPO/quad_task.py
-+++ b/TRPO/quad_task.py
-@@ -12,12 +12,18 @@ class hover(object):
-         # hover_position = np.asarray([0, 0, 0]) # pn = 0, pe = 0, pd = 0
-         if terminal and info=='outrange':
-             print info
--            reward = -10000
-+            reward = -1000
-+            return reward
-+
-+        if terminal and info=="pen_zeta<0":
-+            print info
-+            reward = -1000
-             return reward
- 
-         current_position = states[0:3]
-+        error = current_position - self.hover_position_set
-+        # print error
-         # last_position = state_last[0:3]
--
--        reward = -np.mean((current_position - self.hover_position_set)**2)+200
--
-+        reward = (-np.mean(error**2))/1000
-+        # print reward
-         return reward
-diff --git a/TRPO/quad_task.pyc b/TRPO/quad_task.pyc
-index a78692c..b269f9e 100644
-Binary files a/TRPO/quad_task.pyc and b/TRPO/quad_task.pyc differ
-diff --git a/TRPO/simulator_di.pyc b/TRPO/simulator_di.pyc
-index 89f0d8e..277ffea 100644
-Binary files a/TRPO/simulator_di.pyc and b/TRPO/simulator_di.pyc differ
-diff --git a/TRPO/util.py b/TRPO/util.py
-index 2f217a2..484aad4 100644
---- a/TRPO/util.py
-+++ b/TRPO/util.py
-@@ -36,5 +36,5 @@ def plot_states(states):
-     axes[4, 2].plot(states[:,14])
-     axes[4, 2].set_title('pen_vx')
-     fig.subplots_adjust(hspace=1.4) 
--    plt.show()
-+    # plt.show()
- 
-diff --git a/TRPO/util.pyc b/TRPO/util.pyc
-index c56d309..70a011c 100644
-Binary files a/TRPO/util.pyc and b/TRPO/util.pyc differ
-diff --git a/eval.py b/eval.py
-index b148390..b65223d 100644
---- a/eval.py
-+++ b/eval.py
-@@ -1,20 +1,19 @@
- # This file is aim to train the quadcopter keet at a constant position
- 
- import numpy as np
--import tflearn
- import time
--from simulator import QuadCopter
-+from simulator_di import QuadCopter
- from replay_buffer import ReplayBuffer
- from  util import *
-+from quad_task import *
- import tensorflow as tf
- import tensorflow.contrib.layers as layers
--import sys, os
--
--
-+import tflearn
- 
- # ==========================
- #   Training Parameters
- # =========================
-+PLOT = True
- # Simulation step
- SIM_TIME_STEP = 0.01
- # Max training steps
-@@ -23,14 +22,14 @@ MAX_EPISODES = 50000
- MAX_EP_TIME = 2 # second
- MAX_EP_STEPS = int(MAX_EP_TIME/SIM_TIME_STEP)
- # Explore decay rate
--EXPLORE_INIT = 1
--EXPLORE_DECAY = 0.99
--EXPLORE_MIN = 0.1
-+EXPLORE_INIT = 5
-+EXPLORE_DECAY = 0.999
-+EXPLORE_MIN = 0.01
- 
- # Base learning rate for the Actor network
--ACTOR_LEARNING_RATE = 1e-4
-+ACTOR_LEARNING_RATE = 1e-6
- # Base learning rate for the Critic Network
--CRITIC_LEARNING_RATE = 1e-3
-+CRITIC_LEARNING_RATE = 1e-6
- # Discount factor 
- GAMMA = 0.99
- # Soft target update param
-@@ -42,7 +41,7 @@ TAU = 0.001
- 
- # Directory for storing tensorboard summary results
- SUMMARY_DIR = './results/ddpg/'
--SAVE_STEP = 10
-+SAVE_STEP = 50
- 
- RANDOM_SEED = 1234
- # Size of replay buffer
-@@ -95,29 +94,29 @@ class ActorNetwork(object):
-         self.num_trainable_vars = len(self.network_params) + len(self.target_network_params)
- 
-     def create_actor_network(self): 
--        inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim], name='state')
--        net = tf.layers.batch_normalization(inputs)
--        net = layers.fully_connected(inputs, num_outputs=400, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
--        net = tf.layers.batch_normalization(net)
--
--        net = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
--        net = tf.layers.batch_normalization(net)
--        # net = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.th)
--        out_w = tf.Variable(np.random.randn(300, self.a_dim)*3e-3, dtype=tf.float32, name="out_w")
--        out_b = tf.Variable(tf.zeros([self.a_dim]), dtype=tf.float32, name="out_b")
--        out = tf.tanh(tf.matmul(net, out_w) +out_b)
--        scaled_out = tf.multiply(out, self.action_limit)# Scale output to -action_limit to action_limit
--
--        ## tflearn version
--        # inputs = tflearn.input_data(shape=[None, self.s_dim])
--        # net = tflearn.fully_connected(inputs, 400, activation='relu')
--        # net = tflearn.fully_connected(net, 300, activation='relu')
--        # # Final layer weights are init to Uniform[-3e-3, 3e-3]
--        # w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
--        # out = tflearn.fully_connected(
--        #     net, self.a_dim, activation='tanh', weights_init=w_init)
--        # # Scale output to -action_bound to action_bound
--        # scaled_out = tf.multiply(out, self.action_limit)
-+        # inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim], name='state')
-+        # net = tf.layers.batch_normalization(inputs)
-+        # net = layers.fully_connected(inputs, num_outputs=400 ,activation_fn=tf.nn.relu)
-+        # net = tf.layers.batch_normalization(net)
-+
-+        # net = layers.fully_connected(net, num_outputs=300 ,activation_fn=tf.nn.relu)
-+        # net = tf.layers.batch_normalization(net)
-+        # # net = layers.fully_connected(net, num_outputs=300 ,activation_fn=tf.th)
-+        # out_w = tf.Variable(np.random.randn(300, self.a_dim)*3e-3, dtype=tf.float32, name="out_w")
-+        # out_b = tf.Variable(tf.zeros([self.a_dim]), dtype=tf.float32, name="out_b")
-+        # out = tf.tanh(tf.matmul(net, out_w) + out_b)
-+        # scaled_out = tf.multiply(out, self.action_limit)# Scale output to -action_limit to action_limit
-+
-+        # tflearn version
-+        inputs = tflearn.input_data(shape=[None, self.s_dim])
-+        net = tflearn.fully_connected(inputs, 400, activation='relu')
-+        net = tflearn.fully_connected(net, 300, activation='relu')
-+        # Final layer weights are init to Uniform[-3e-3, 3e-3]
-+        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
-+        out = tflearn.fully_connected(
-+            net, self.a_dim, activation='sigmoid', weights_init=w_init)
-+        # Scale output to -action_bound to action_bound
-+        scaled_out = tf.multiply(out, self.action_limit*2) - self.action_limit
- 
- 
-         return inputs, out, scaled_out 
-@@ -185,41 +184,40 @@ class CriticNetwork(object):
-         self.action_grads = tf.gradients(self.out, self.action)
- 
-     def create_critic_network(self):
--        inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim])
--        action = tf.placeholder(dtype=tf.float32, shape=[None, self.a_dim])
-+        # inputs = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim])
-+        # action = tf.placeholder(dtype=tf.float32, shape=[None, self.a_dim])
- 
--        net = layers.fully_connected(inputs, num_outputs=400, weights_initializer=layers.xavier_initializer() ,activation_fn=tf.nn.relu)
--        # Add the action tensor in the 2nd hidden layer
--        # Use two temp layers to get the corresponding weights and biases 
--        t1 = layers.fully_connected(net, num_outputs=300, weights_initializer=layers.xavier_initializer(), activation_fn=None)
--        t2 = layers.fully_connected(action, num_outputs=300, weights_initializer=layers.xavier_initializer(), activation_fn=None)
-+        # net = layers.fully_connected(inputs, num_outputs=400 ,activation_fn=tf.nn.relu)
-+        # # Add the action tensor in the 2nd hidden layer
-+        # # Use two temp layers to get the corresponding weights and biases 
-+        # t1 = layers.fully_connected(net, num_outputs=300, activation_fn=None)
-+        # t2 = layers.fully_connected(action, num_outputs=300, activation_fn=None)
- 
--        net = tf.nn.relu(t1 + t2)
--        net = tf.layers.batch_normalization(net)
--        out_w = tf.Variable(np.random.randn(300, 1)*3e-3, dtype=tf.float32)
--        out_b = tf.Variable(tf.zeros([1]), dtype=tf.float32, name="out_b")
-+        # net = tf.nn.relu(t1 + t2)
-+        # net = tf.layers.batch_normalization(net)
-+        # out_w = tf.Variable(np.random.randn(300, 1)*3e-3, dtype=tf.float32)
-+        # out_b = tf.Variable(tf.zeros([1]), dtype=tf.float32, name="out_b")
- 
--        out = tf.matmul(net, out_w) + out_b
-+        # out = tf.matmul(net, out_w) + out_b
- 
--        # out = layers.fully_connected(net, num_outputs=1, weights_initializer=layers.xavier_initializer(), activation_fn=None)
- 
-         # tflearn version
--        # inputs = tflearn.input_data(shape=[None, self.s_dim])
--        # action = tflearn.input_data(shape=[None, self.a_dim])
--        # net = tflearn.fully_connected(inputs, 400, activation='relu')
-+        inputs = tflearn.input_data(shape=[None, self.s_dim])
-+        action = tflearn.input_data(shape=[None, self.a_dim])
-+        net = tflearn.fully_connected(inputs, 400, activation='relu')
- 
-         # Add the action tensor in the 2nd hidden layer
-         # Use two temp layers to get the corresponding weights and biases
--        # t1 = tflearn.fully_connected(net, 300)
--        # t2 = tflearn.fully_connected(action, 300)
-+        t1 = tflearn.fully_connected(net, 300)
-+        t2 = tflearn.fully_connected(action, 300)
- 
--        # net = tflearn.activation(
--        #     tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')
-+        net = tflearn.activation(
-+            tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')
- 
--        # # linear layer connected to 1 output representing Q(s,a)
--        # # Weights are init to Uniform[-3e-3, 3e-3]
--        # w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
--        # out = tflearn.fully_connected(net, 1, weights_init=w_init)
-+        # linear layer connected to 1 output representing Q(s,a)
-+        # Weights are init to Uniform[-3e-3, 3e-3]
-+        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
-+        out = tflearn.fully_connected(net, 1, weights_init=w_init)
-         return inputs, action, out
- 
-     def train(self, inputs, action, predicted_q_value):
-@@ -255,7 +253,7 @@ class CriticNetwork(object):
- # ===========================
- def build_summaries(): 
-     success_rate = tf.Variable(0.)
--    tf.summary.scalar("Success Rate", success_rate)
-+    tf.summary.scalar("Reward ", success_rate)
-     episode_ave_max_q = tf.Variable(0.)
-     tf.summary.scalar("Qmax Value", episode_ave_max_q)
- 
-@@ -276,35 +274,13 @@ def count_parameters():
-     print("total_parameters:", total_parameters)
- 
- 
--# ===========================
--#   Reward functions
--# ===========================
--def reward_function_hover_decorator(hover_position_set):
--
--    def reward_function_hover(states, terminal, info):
--        # this function is aimed to let the quadcopter hover in a certain position.
--        # e.g
--        # hover_position = np.asarray([0, 0, 0]) # pn = 0, pe = 0, pd = 0
--
--        if terminal:
--            reward = -500
--            print "terminated " , info
--        else:
--            current_position = states[0:3]
--            # reward function = -MSE(current_position, hover_position)
--            reward = -np.mean((current_position - hover_position_set)**2) + 200
--            # print reward
--
--        return reward
--
--    return reward_function_hover
- 
- # ===========================
- #   Agent Training
- # ===========================
- 
- 
--def train(sess, env, actor, critic, reward_fc):
-+def train(sess, env, actor, critic, task):
- 
-     # Set up summary Ops
-     summary_ops, summary_vars = build_summaries()
-@@ -325,42 +301,102 @@ def train(sess, env, actor, critic, reward_fc):
-     else:
-         print ("Could not find old network weights")
- 
--
--    states = np.zeros([MAX_EP_STEPS, env.stateSpace])
--
--    for j in xrange(MAX_EP_STEPS):
--
--        a = actor.predict(np.reshape(s, (1, 16)))
--
--        s2, terminal, info = env.step(a[0])
--        states[j] = s2
--
--        r = reward_fc(s2, terminal, info) # calculate reward basec on s2
--
--        s = s2
--        ep_reward += r
--
--        if terminal or j == MAX_EP_STEPS-1 or r < -10000:
--
--            plot_states(states)
--
--            print s[0:3]
--
--            last_epreward = ep_reward
--            print '| Reward: %.2f' % int(ep_reward/(j+1)), " | Episode", i, \
--                    '| Qmax: %.4f' % (ep_ave_max_q / float(j+1))
--
-+    # Initialize target network weights
-+    actor.update_target_network()
-+    critic.update_target_network()
-+    count_parameters()
-+
-+    # Initialize replay memory
-+    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)
-+    tic = time.time()
-+    last_epreward = 0 
-+    i = global_step.eval()
-+
-+    # plot
-+    if PLOT:
-+        # plt.ion()
-+        fig1 = plt.figure()
-+        plot1 = fig1.add_subplot(131)
-+        plot2 = fig1.add_subplot(132)
-+        plot3 = fig1.add_subplot(133)
-+
-+
-+    while True:
-+        i += 1
-+        if i > MAX_EPISODES:
-             break
-+        print ("Iteration: ", i)
-+
-+        s = env.reset()
-+
-+        ep_reward = 0
-+        ep_ave_max_q = 0
-+        states = np.zeros([MAX_EP_STEPS+1, env.stateSpace])
-+        actions = np.zeros([MAX_EP_STEPS+1, env.actionSpace])
-+
-+        if i % SAVE_STEP == 0: # save check point every xx episode
-+            # sess.run(global_step.assign(i))
-+            save_path = saver.save(sess, SUMMARY_DIR + "model.ckpt" , global_step = i)
-+            print("Model saved in file: %s" % save_path)
-+
-+        for j in xrange(MAX_EP_STEPS+1):
-+
-+            # Added exploration noise
-+            # exp = np.random.rand(1, 4) * explore * env.actionLimit
-+            # exp = np.random.rand(1, actor.a_dim) * explore * env.actionLimit*2 - explore * env.actionLimit 
-+
-+            a = actor.predict(np.reshape(s, (1, 16)))
-+            act = np.reshape(a, (actor.a_dim,))
-+            print act
-+            # print act.shape
-+            act = [act[0], act[1], act[2], 0, 0, 0]
-+            # print act
-+            # a = actor.predict(np.reshape(s, (1, 16))) + (1. / (1. + i))
-+            s2, terminal, info = env.step(act)
-+            # print 's', s
-+            # print 's2', s2
-+            # print j
-+            # print "action: ", a[0]
-+            # print "state: ", s2
-+            states[j] = s2
-+            actions[j] = act
-+            r = task.reward(s2, terminal, info) # calculate reward basec on s2
-+            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r, \
-+                terminal, np.reshape(s2, (actor.s_dim,)))
-+
-+
-+            ep_reward += r
-+            if terminal:
-+                # if i > 30:
-+                #     plot_states(states)
-+                if PLOT:
-+                    plot1.plot(states[:,0])    
-+                    plot2.plot(states[:,1])  
-+                    plot3.plot(states[:,2])    
-+
-+                    plt.show()
-+
-+                print s[0:3]
-+                time_gap = time.time() - tic
-+
-+                print '| Reward: %.2f' % (ep_reward/(j+1)), " | Episode", i, \
-+                        '| Qmax: %.4f' % (ep_ave_max_q / float(j+1)), ' | Time: %.2f' %(time_gap)
-+                tic = time.time()
-+
-+                break
-+            s = np.copy(s2)
-+        break
- 
- 
- def main(_):
-         
-         np.random.seed(RANDOM_SEED)
-         tf.set_random_seed(RANDOM_SEED)
--        env  = QuadCopter(SIM_TIME_STEP, inverted_pendulum=False)
-+        env  = QuadCopter(SIM_TIME_STEP, max_time = MAX_EP_TIME, inverted_pendulum=False)
- 
-         state_dim = env.stateSpace
--        action_dim = env.actionSpace
-+        # action_dim = env.actionSpace
-+        action_dim = 3
-         action_limit = env.actionLimit
- 
-         print("Quadcopter created")
-@@ -370,8 +406,8 @@ def main(_):
-         print('max time: ', MAX_EP_TIME)
-         print('max step: ',MAX_EP_STEPS)        
- 
--        hover_position = np.asarray([0, 0, -50])
--        reward_fc = reward_function_hover_decorator(hover_position)
-+        hover_position = np.asarray([0, 10, -10])
-+        task = hover(hover_position)
-         config = tf.ConfigProto()
-         config.gpu_options.allow_growth = True
- 
-@@ -382,7 +418,7 @@ def main(_):
-             critic = CriticNetwork(sess, state_dim, action_dim, \
-                 CRITIC_LEARNING_RATE, TAU, actor.get_num_trainable_vars())
- 
--            train(sess, env, actor, critic, reward_fc)
-+            train(sess, env, actor, critic, task)
- 
- if __name__ == '__main__':
-     tf.app.run()
-diff --git a/results/ddpg/checkpoint b/results/ddpg/checkpoint
-deleted file mode 100644
-index 66efd8c..0000000
---- a/results/ddpg/checkpoint
-+++ /dev/null
-@@ -1,5 +0,0 @@
--model_checkpoint_path: "model.ckpt-200"
--all_model_checkpoint_paths: "model.ckpt-50"
--all_model_checkpoint_paths: "model.ckpt-100"
--all_model_checkpoint_paths: "model.ckpt-150"
--all_model_checkpoint_paths: "model.ckpt-200"
-diff --git a/results/ddpg/events.out.tfevents.1496824529.Zhenghaos-MacBook-Pro.local b/results/ddpg/events.out.tfevents.1496824529.Zhenghaos-MacBook-Pro.local
-deleted file mode 100644
-index d53a0d5..0000000
-Binary files a/results/ddpg/events.out.tfevents.1496824529.Zhenghaos-MacBook-Pro.local and /dev/null differ
-diff --git a/results/ddpg/events.out.tfevents.1496854151.campus-102-130.ucdavis.edu b/results/ddpg/events.out.tfevents.1496854151.campus-102-130.ucdavis.edu
-deleted file mode 100644
-index ec59794..0000000
-Binary files a/results/ddpg/events.out.tfevents.1496854151.campus-102-130.ucdavis.edu and /dev/null differ
-diff --git a/results/ddpg/events.out.tfevents.1496854187.campus-102-130.ucdavis.edu b/results/ddpg/events.out.tfevents.1496854187.campus-102-130.ucdavis.edu
-deleted file mode 100644
-index 5efb2c7..0000000
-Binary files a/results/ddpg/events.out.tfevents.1496854187.campus-102-130.ucdavis.edu and /dev/null differ
-diff --git a/results/ddpg/events.out.tfevents.1496854203.campus-102-130.ucdavis.edu b/results/ddpg/events.out.tfevents.1496854203.campus-102-130.ucdavis.edu
-deleted file mode 100644
-index 0a4b46a..0000000
-Binary files a/results/ddpg/events.out.tfevents.1496854203.campus-102-130.ucdavis.edu and /dev/null differ
-diff --git a/results/ddpg/events.out.tfevents.1496854323.campus-102-130.ucdavis.edu b/results/ddpg/events.out.tfevents.1496854323.campus-102-130.ucdavis.edu
-deleted file mode 100644
-index 4f27dfa..0000000
-Binary files a/results/ddpg/events.out.tfevents.1496854323.campus-102-130.ucdavis.edu and /dev/null differ
-diff --git a/results/ddpg/events.out.tfevents.1496854344.campus-102-130.ucdavis.edu b/results/ddpg/events.out.tfevents.1496854344.campus-102-130.ucdavis.edu
-deleted file mode 100644
-index 3b58f10..0000000
-Binary files a/results/ddpg/events.out.tfevents.1496854344.campus-102-130.ucdavis.edu and /dev/null differ
-diff --git a/results/ddpg/events.out.tfevents.1496859348.campus-102-130.ucdavis.edu b/results/ddpg/events.out.tfevents.1496859348.campus-102-130.ucdavis.edu
-deleted file mode 100644
-index 90aca94..0000000
-Binary files a/results/ddpg/events.out.tfevents.1496859348.campus-102-130.ucdavis.edu and /dev/null differ
-diff --git a/results/ddpg/model.ckpt-100.data-00000-of-00001 b/results/ddpg/model.ckpt-100.data-00000-of-00001
-deleted file mode 100644
-index d6d8891..0000000
-Binary files a/results/ddpg/model.ckpt-100.data-00000-of-00001 and /dev/null differ
-diff --git a/results/ddpg/model.ckpt-100.index b/results/ddpg/model.ckpt-100.index
-deleted file mode 100644
-index 0821ad3..0000000
-Binary files a/results/ddpg/model.ckpt-100.index and /dev/null differ
-diff --git a/results/ddpg/model.ckpt-100.meta b/results/ddpg/model.ckpt-100.meta
-deleted file mode 100644
-index 174b953..0000000
-Binary files a/results/ddpg/model.ckpt-100.meta and /dev/null differ
-diff --git a/results/ddpg/model.ckpt-150.data-00000-of-00001 b/results/ddpg/model.ckpt-150.data-00000-of-00001
-deleted file mode 100644
-index 9d9ed2f..0000000
-Binary files a/results/ddpg/model.ckpt-150.data-00000-of-00001 and /dev/null differ
-diff --git a/results/ddpg/model.ckpt-150.index b/results/ddpg/model.ckpt-150.index
-deleted file mode 100644
-index 02cec58..0000000
-Binary files a/results/ddpg/model.ckpt-150.index and /dev/null differ
-diff --git a/results/ddpg/model.ckpt-150.meta b/results/ddpg/model.ckpt-150.meta
-deleted file mode 100644
-index 174b953..0000000
-Binary files a/results/ddpg/model.ckpt-150.meta and /dev/null differ
-diff --git a/results/ddpg/model.ckpt-200.data-00000-of-00001 b/results/ddpg/model.ckpt-200.data-00000-of-00001
-deleted file mode 100644
-index 593fffa..0000000
-Binary files a/results/ddpg/model.ckpt-200.data-00000-of-00001 and /dev/null differ
-diff --git a/results/ddpg/model.ckpt-200.index b/results/ddpg/model.ckpt-200.index
-deleted file mode 100644
-index 04c3c41..0000000
-Binary files a/results/ddpg/model.ckpt-200.index and /dev/null differ
-diff --git a/results/ddpg/model.ckpt-200.meta b/results/ddpg/model.ckpt-200.meta
-deleted file mode 100644
-index 174b953..0000000
-Binary files a/results/ddpg/model.ckpt-200.meta and /dev/null differ
-diff --git a/results/ddpg/model.ckpt-50.data-00000-of-00001 b/results/ddpg/model.ckpt-50.data-00000-of-00001
-deleted file mode 100644
-index 756c221..0000000
-Binary files a/results/ddpg/model.ckpt-50.data-00000-of-00001 and /dev/null differ
-diff --git a/results/ddpg/model.ckpt-50.index b/results/ddpg/model.ckpt-50.index
-deleted file mode 100644
-index 85725b8..0000000
-Binary files a/results/ddpg/model.ckpt-50.index and /dev/null differ
-diff --git a/results/ddpg/model.ckpt-50.meta b/results/ddpg/model.ckpt-50.meta
-deleted file mode 100644
-index 174b953..0000000
-Binary files a/results/ddpg/model.ckpt-50.meta and /dev/null differ
-diff --git a/simulator.py b/simulator.py
-index fcb89ad..4e30424 100644
---- a/simulator.py
-+++ b/simulator.py
-@@ -213,8 +213,8 @@ class QuadCopter(object):
-                       + (pen_ydot**2.0*pen_y**3.0+2*pen_ydot*pen_xdot*pen_y**2.0*pen_x+pen_xdot**2.0*pen_x**2.0*pen_y)/(pen_zeta**4.0) \
-                       - (pen_y*(zddot+self.gravity))/(pen_zeta))
-             pen_vxdot = (pen_alpha - pen_beta*pen_x*pen_y/((self.pen_l**2.0-pen_y**2.0)*pen_zeta**2.0)) \
--                      * (1 - (pen_x**2.0*pen_y**2.0)/((self.pen_l**2.0-pen_y**2.0)**2.0*pen_zeta**4.0))
--            pen_vydot = pen_beta - (pen_vxdot*pen_x*pen_y)/(self.pen_l**2.0-pen_x**2.0)
-+                      * (1.0 - (pen_x**2.0*pen_y**2.0)/((self.pen_l**2.0-pen_y**2.0)**2.0*pen_zeta**4.0))
-+            pen_vydot = pen_beta - (pen_vxdot*pen_x*pen_y)/((self.pen_l**2.0-pen_x**2.0)*pen_zeta**2.0)
-         else:
-             pen_zeta  = 0
-             pen_xdot  = 0
-diff --git a/simulator.pyc b/simulator.pyc
-index 24d8053..b410f40 100644
-Binary files a/simulator.pyc and b/simulator.pyc differ
-diff --git a/simulator_1.pyc b/simulator_1.pyc
-deleted file mode 100644
-index 6efb32d..0000000
-Binary files a/simulator_1.pyc and /dev/null differ
-diff --git a/simulator_di.py b/simulator_di.py
-index 88ab097..846aef7 100644
---- a/simulator_di.py
-+++ b/simulator_di.py
-@@ -204,6 +204,9 @@ class QuadCopter(object):
-      # inverted pendulum dynamics
-         if self.inverted_pendulum: 
-             pen_zeta  = np.sqrt(self.pen_l**2.0 - pen_x**2.0 - pen_y**2.0)
-+            if pen_zeta <=0:
-+                self.terminated = True
-+                self.info = "pen_zeta<0"
-             pen_xdot  = pen_vx
-             pen_ydot  = pen_vy
-             pen_alpha = (-pen_zeta**2.0/(pen_zeta**2.0+pen_x**2.0)) * (xddot+(pen_xdot**2.0*pen_x+pen_ydot**2.0*pen_x)/(pen_zeta**2.0) \
-@@ -213,8 +216,8 @@ class QuadCopter(object):
-                       + (pen_ydot**2.0*pen_y**3.0+2*pen_ydot*pen_xdot*pen_y**2.0*pen_x+pen_xdot**2.0*pen_x**2.0*pen_y)/(pen_zeta**4.0) \
-                       - (pen_y*(zddot+self.gravity))/(pen_zeta))
-             pen_vxdot = (pen_alpha - pen_beta*pen_x*pen_y/((self.pen_l**2.0-pen_y**2.0)*pen_zeta**2.0)) \
--                      * (1 - (pen_x**2.0*pen_y**2.0)/((self.pen_l**2.0-pen_y**2.0)**2.0*pen_zeta**4.0))
--            pen_vydot = pen_beta - (pen_vxdot*pen_x*pen_y)/(self.pen_l**2.0-pen_x**2.0)
-+                      * (1.0 - (pen_x**2.0*pen_y**2.0)/((self.pen_l**2.0-pen_y**2.0)**2.0*pen_zeta**4.0))
-+            pen_vydot = pen_beta - (pen_vxdot*pen_x*pen_y)/((self.pen_l**2.0-pen_x**2.0)*pen_zeta**2.0)
-         else:
-             pen_zeta  = 0
-             pen_xdot  = 0
-@@ -236,8 +239,8 @@ class QuadCopter(object):
- 
- 
-     def step(self, uu):
--        terminated = False
--        info = 'normal'
-+        self.terminated = False
-+        self.info = 'normal'
- 
-         # delta   = np.asarray(delta)*3.1416/180
-         # delta_f = delta[0]
-@@ -314,19 +317,19 @@ class QuadCopter(object):
-             self.r = -self.r_max
- 
-         if self.time > self.max_time:
--            terminated = True
--            info = 'timeout'   
-+            self.terminated = True
-+            self.info = 'timeout'   
-     # # Fail condition check
-     #     if self.pd > 0:
--    #         terminated = True
--    #         info = 'crash'   
-+    #         self.terminated = True
-+    #         self.info = 'crash'   
- 
-         # print 'Time = %f' %self.time
-         self.states = np.asarray([self.pn, self.pe, self.pd, self.u, self.v, self.w, self.phi, self.theta, self.psi,
-                                   self.p,  self.q,  self.r,  self.pen_x,  self.pen_y,  self.pen_vx,  self.pen_vy])
--        if  terminated:
-+        if  self.terminated:
-             self.reset()
- 
--        return self.states, terminated, info
-+        return self.states, self.terminated, self.info
- 
- # end class
-\ No newline at end of file
-diff --git a/simulator_direct_input.pyc b/simulator_direct_input.pyc
-deleted file mode 100644
-index 5fc27de..0000000
-Binary files a/simulator_direct_input.pyc and /dev/null differ
-diff --git a/train_quadcopter_di.py b/train_quadcopter_di.py
-index c8e8292..104ca84 100644
---- a/train_quadcopter_di.py
-+++ b/train_quadcopter_di.py
-@@ -2,7 +2,7 @@
- 
- import numpy as np
- import time
--from simulator_direct_input import QuadCopter
-+from simulator_di import QuadCopter
- from replay_buffer import ReplayBuffer
- from  util import *
- from quad_task import *
-@@ -13,6 +13,7 @@ import tflearn
- # ==========================
- #   Training Parameters
- # =========================
-+PLOT = False
- # Simulation step
- SIM_TIME_STEP = 0.01
- # Max training steps
-@@ -21,14 +22,14 @@ MAX_EPISODES = 50000
- MAX_EP_TIME = 2 # second
- MAX_EP_STEPS = int(MAX_EP_TIME/SIM_TIME_STEP)
- # Explore decay rate
--EXPLORE_INIT = 0.5
--EXPLORE_DECAY = 0.99
--EXPLORE_MIN = 0.005
-+EXPLORE_INIT = 0.1
-+EXPLORE_DECAY = 0.999
-+EXPLORE_MIN = 0.01
- 
- # Base learning rate for the Actor network
--ACTOR_LEARNING_RATE = 1e-5
-+ACTOR_LEARNING_RATE = 1e-6
- # Base learning rate for the Critic Network
--CRITIC_LEARNING_RATE = 1e-4
-+CRITIC_LEARNING_RATE = 1e-6
- # Discount factor 
- GAMMA = 0.99
- # Soft target update param
-@@ -113,9 +114,9 @@ class ActorNetwork(object):
-         # Final layer weights are init to Uniform[-3e-3, 3e-3]
-         w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
-         out = tflearn.fully_connected(
--            net, self.a_dim, activation='tanh', weights_init=w_init)
-+            net, self.a_dim, activation='sigmoid', weights_init=w_init)
-         # Scale output to -action_bound to action_bound
--        scaled_out = tf.multiply(out, self.action_limit)
-+        scaled_out = tf.multiply(out, self.action_limit*2) - self.action_limit
- 
- 
-         return inputs, out, scaled_out 
-@@ -311,6 +312,14 @@ def train(sess, env, actor, critic, task):
-     last_epreward = 0 
-     i = global_step.eval()
- 
-+    # plot
-+    if PLOT:
-+        plt.ion()
-+        fig1 = plt.figure()
-+        plot1 = fig1.add_subplot(121)
-+        plot2 = fig1.add_subplot(122)
-+
-+
-     while True:
-         i += 1
-         if i > MAX_EPISODES:
-@@ -324,6 +333,7 @@ def train(sess, env, actor, critic, task):
-         ep_reward = 0
-         ep_ave_max_q = 0
-         states = np.zeros([MAX_EP_STEPS+1, env.stateSpace])
-+        actions = np.zeros([MAX_EP_STEPS+1, env.actionSpace])
- 
-         if i % SAVE_STEP == 0: # save check point every xx episode
-             # sess.run(global_step.assign(i))
-@@ -334,20 +344,26 @@ def train(sess, env, actor, critic, task):
- 
-             # Added exploration noise
-             # exp = np.random.rand(1, 4) * explore * env.actionLimit
--            exp = np.random.rand(1, 6) * explore * env.actionLimit
--
--            a = actor.predict(np.reshape(s, (1, 16))) + exp
--            # a = [[2,2,2,2]]
--     
-+            exp = np.random.rand(1, actor.a_dim) * explore * env.actionLimit*2 - explore * env.actionLimit 
-+            print "exp: ", exp
-+            
-+            a = actor.predict(np.reshape(s, (1, 16)))
-+            print "raw a: ", a
-+            a += exp
-+            print "a: ", a
-+            act = np.reshape(a, (actor.a_dim,))
-+            # print act.shape
-+            act = [act[0], act[1], act[2], 0, 0, 0]
-+            # print act
-             # a = actor.predict(np.reshape(s, (1, 16))) + (1. / (1. + i))
--            s2, terminal, info = env.step(a[0])
-+            s2, terminal, info = env.step(act)
-             # print 's', s
-             # print 's2', s2
-             # print j
-             # print "action: ", a[0]
-             # print "state: ", s2
-             states[j] = s2
--
-+            actions[j] = act
-             r = task.reward(s2, terminal, info) # calculate reward basec on s2
-             replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r, \
-                 terminal, np.reshape(s2, (actor.s_dim,)))
-@@ -386,6 +402,10 @@ def train(sess, env, actor, critic, task):
-             if terminal:
-                 # if i > 30:
-                 #     plot_states(states)
-+                if PLOT:
-+                    plot1.plot(states[:,2])    
-+                    plot2.plot(states[:,5])  
-+                    plt.pause(0.001)
- 
-                 print s[0:3]
-                 time_gap = time.time() - tic
-@@ -413,7 +433,8 @@ def main(_):
-         env  = QuadCopter(SIM_TIME_STEP, max_time = MAX_EP_TIME, inverted_pendulum=False)
- 
-         state_dim = env.stateSpace
--        action_dim = env.actionSpace
-+        # action_dim = env.actionSpace
-+        action_dim = 3
-         action_limit = env.actionLimit
- 
-         print("Quadcopter created")
-@@ -423,7 +444,7 @@ def main(_):
-         print('max time: ', MAX_EP_TIME)
-         print('max step: ',MAX_EP_STEPS)        
- 
--        hover_position = np.asarray([0, 0, -10])
-+        hover_position = np.asarray([0, 10, -10])
-         task = hover(hover_position)
-         config = tf.ConfigProto()
-         config.gpu_options.allow_growth = True
-diff --git a/util.py b/util.py
-index 2f217a2..484aad4 100644
---- a/util.py
-+++ b/util.py
-@@ -36,5 +36,5 @@ def plot_states(states):
-     axes[4, 2].plot(states[:,14])
-     axes[4, 2].set_title('pen_vx')
-     fig.subplots_adjust(hspace=1.4) 
--    plt.show()
-+    # plt.show()
- 
-diff --git a/util.pyc b/util.pyc
-index c3773d4..ffba700 100644
-Binary files a/util.pyc and b/util.pyc differ
diff --git a/TRPO/quad/log.txt b/TRPO/quad/log.txt
index a8e860a..e69de29 100644
--- a/TRPO/quad/log.txt
+++ b/TRPO/quad/log.txt
@@ -1,33 +0,0 @@
-EpRewMean	EpLenMean	KLOldNew	Entropy	EVBefore	EVAfter	TimestepsSoFar
--165.246831627	101.0	2.38419e-07	-3.32956e-05	0.0	0.467209659448	2525
--166.767080253	101.0	9.62019e-05	-0.000202157	0.198709138699	0.464214919111	5050
--169.431402699	101.0	0.00281399	-0.000533843	0.270153526797	0.598051386352	7575
--176.746876404	101.0	0.0113655	-0.00101985	0.178127876778	0.584747995769	10100
--172.410385368	101.0	0.0110359	-0.00170964	0.270550900475	0.598692317165	12625
--166.704478184	101.0	0.00602722	-0.00302632	-0.00839294329504	0.520205006376	15150
--167.937614127	101.0	0.00295693	-0.00378175	0.405225905237	0.516126483439	17675
--168.0897291	101.0	0.00271034	-0.00530858	0.35866903248	0.587193734399	20200
--174.949512891	101.0	0.00212401	-0.0103128	-1.19442235943	0.540257167066	22725
--166.375062738	101.0	0.00194681	-0.0122592	0.192033103361	0.471467669692	25250
--166.8474648	101.0	0.00125122	-0.0190005	0.407359726531	0.607906515189	27775
--173.661500036	101.0	0.00118029	-0.0188681	-0.122316789109	0.444310654386	30300
--174.135032758	101.0	0.000611901	-0.0244628	0.423140113584	0.52911591351	32825
--176.057699035	101.0	0.000745833	-0.0336524	0.218330012851	0.604188472176	35350
--173.772916701	101.0	0.00051415	-0.0851259	-0.228111304	0.572344814097	37875
--168.217162399	101.0	0.000291824	-0.110239	0.306840951204	0.534323470763	40400
--172.035097561	101.0	0.000217974	-0.238259	0.0986920338222	0.461180085601	42925
--189.369975304	101.0	0.00065136	-0.586896	0.326724012215	0.567650112991	45450
--180.778422353	101.0	0.00202698	-1.73204	-0.745876695663	0.456808745987	47975
--194.512014625	101.0	0.0106671	-2.40895	-0.0911332899431	0.351591276207	50500
--202.686974326	101.0	0.0304315	-8.69372	0.171402356623	0.509267025119	53025
--158.355264846	101.0	0.0210206	-28.7119	-1.21940146371	0.779984882552	55550
--139.495224266	101.0	0.0553832	-23.5918	0.728619008567	0.895036493403	58075
--186.962609522	101.0	0.0168464	-22.1669	-0.345979629433	0.793022948275	60600
--198.671406305	101.0	0.00668889	-30.6727	0.286911145794	0.653131253923	63125
--199.509956747	101.0	0.00148022	-28.35	0.436755999538	0.570692434729	65650
--208.610569359	101.0	0.000550091	-33.1111	0.46867020068	0.570167885448	68175
--216.659808986	101.0	0.000713408	-33.4152	0.394114764897	0.459938644987	70700
--218.798797343	101.0	0.0153955	-58.1258	0.523797918184	0.670236102451	73225
--225.463637573	101.0	0.00164938	-76.1834	0.472778613482	0.577210250631	75750
--223.71772432	101.0	0.000738978	-87.0942	0.442712560707	0.54707516545	78275
--221.281357065	101.0	0.000689328	-92.6442	0.49478558858	0.64439204448	80800
diff --git a/universe-starter-agent b/universe-starter-agent
--- a/universe-starter-agent
+++ b/universe-starter-agent
@@ -1 +1 @@
-Subproject commit f16f37d9d3bc8146cf68a75557e1ba89824b7e54
+Subproject commit f16f37d9d3bc8146cf68a75557e1ba89824b7e54-dirty
